{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 고잉 deeper NLP 6장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WEAT(Word Embedding Association Test)\n",
    "\n",
    "워드 임베딩에 내포된 편향성을 정량적으로 측정할 방법 \n",
    "\n",
    "코사인 유사도를 통해서 계산함. 두 벡터의 코사인값을 이용하여 각도를 계산함\n",
    "\n",
    "[논문 원본](https://arxiv.org/pdf/1608.07187.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(i, j):\n",
    "    return dot(i, j.T)/(norm(i)*norm(j))\n",
    "\n",
    "def s(w, A, B):\n",
    "    c_a = cos_sim(w, A)\n",
    "    c_b = cos_sim(w, B)\n",
    "    mean_A = np.mean(c_a, axis=-1)\n",
    "    mean_B = np.mean(c_b, axis=-1)\n",
    "    return mean_A - mean_B#, c_a, c_b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dot prodcut과 magnitude 계산 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.482\n"
     ]
    }
   ],
   "source": [
    "def weat_score(X, Y, A, B):\n",
    "    \n",
    "    s_X = s(X, A, B)\n",
    "    s_Y = s(Y, A, B)\n",
    "\n",
    "    mean_X = np.mean(s_X)\n",
    "    mean_Y = np.mean(s_Y)\n",
    "    \n",
    "    std_dev = np.std(np.concatenate([s_X, s_Y], axis=0))\n",
    "    \n",
    "    return  (mean_X-mean_Y)/std_dev\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s_X(X,A,B)는 단어 x가 두 attribute 셋 A,B에 속한 단어들과의 유사도의 평균값이 얼마나 차이나는지를 측정하게됨.\n",
    "\n",
    "즉 s(X,A,B)는 개별 단어가 개념축 A-B에 대해서 가지는 편향성을 계산한것이 되고, 클수록 편향성을 가지는것임\n",
    "\n",
    "여기다가 std_dev를 통해 표전편차로 나누어 normalize한 값임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir = '~/aiffel/weat' \n",
    "model_dir = os.path.join(data_dir, 'GoogleNews-vectors-negative300.bin')\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# 50만개의 단어만 활용합니다. 메모리가 충분하다면 limit 파라미터값을 생략하여 300만개를 모두 활용할 수 있습니다. \n",
    "w2v = KeyedVectors.load_word2vec_format(model_dir, binary=True, limit=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x7f3d28682a10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000\n",
      "300\n",
      "(500000, 300)\n"
     ]
    }
   ],
   "source": [
    "print(len(w2v.vocab))   # Gensim 3.X 버전까지는 w2v.vocab을 직접 접근할 수 있습니다. \n",
    "# print(len(w2v.index_to_key))   # Gensim 4.0부터는 index_to_key를 활용해 vocab size를 알 수 있습니다. \n",
    "print(len(w2v['I']))                    # 혹은 단어를 key로 직접 vector를 얻을 수 있습니다. \n",
    "print(w2v.vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w2v에는 limit로 지정한 갯수의 단어가 있고 각 단어는 300차원으로 구성되어있음.\n",
    "\n",
    "실제로 happy의 단어 벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.18798828e-04,  1.60156250e-01,  1.60980225e-03,  2.53906250e-02,\n",
       "        9.91210938e-02, -8.59375000e-02,  3.24218750e-01, -2.17285156e-02,\n",
       "        1.34765625e-01,  1.10351562e-01, -1.04980469e-01, -2.90527344e-02,\n",
       "       -2.38037109e-02, -4.02832031e-02, -3.68652344e-02,  2.32421875e-01,\n",
       "        3.20312500e-01,  1.01074219e-01,  5.83496094e-02, -2.91824341e-04,\n",
       "       -3.29589844e-02,  2.11914062e-01,  4.32128906e-02, -8.59375000e-02,\n",
       "        2.81250000e-01, -1.78222656e-02,  3.79943848e-03, -1.71875000e-01,\n",
       "        2.06054688e-01, -1.85546875e-01,  3.73535156e-02, -1.21459961e-02,\n",
       "        2.04101562e-01, -3.80859375e-02,  3.61328125e-02, -8.15429688e-02,\n",
       "        8.44726562e-02,  9.37500000e-02,  1.44531250e-01,  7.42187500e-02,\n",
       "        2.51953125e-01, -7.91015625e-02,  8.69140625e-02,  1.58691406e-02,\n",
       "        1.09375000e-01, -2.23632812e-01, -5.15747070e-03,  1.68945312e-01,\n",
       "       -1.36718750e-01, -2.51464844e-02, -3.85742188e-02, -1.33056641e-02,\n",
       "        1.38671875e-01,  1.76757812e-01,  1.10351562e-01,  1.51367188e-01,\n",
       "        7.86132812e-02, -1.69921875e-01,  1.20605469e-01, -4.37500000e-01,\n",
       "       -4.32128906e-02,  1.34765625e-01, -3.45703125e-01,  9.13085938e-02,\n",
       "        4.71191406e-02,  9.66796875e-02, -1.61132812e-02, -4.71191406e-02,\n",
       "       -4.68750000e-02,  1.37695312e-01,  9.96093750e-02,  4.49218750e-02,\n",
       "       -2.49023438e-02,  1.58203125e-01, -3.57421875e-01, -1.21093750e-01,\n",
       "        1.15722656e-01,  9.08203125e-02,  1.40625000e-01,  1.60156250e-01,\n",
       "       -4.42504883e-03,  5.34667969e-02,  2.28515625e-01,  1.88476562e-01,\n",
       "       -3.88183594e-02, -2.53906250e-01, -1.74804688e-01,  9.81445312e-02,\n",
       "        1.08642578e-02,  1.41601562e-01,  7.81250000e-03,  1.36718750e-01,\n",
       "       -2.08007812e-01, -3.41796875e-02, -2.50000000e-01,  1.25976562e-01,\n",
       "        1.57226562e-01,  3.31115723e-03, -1.51367188e-01, -6.98242188e-02,\n",
       "       -1.40625000e-01,  2.06054688e-01, -3.54003906e-02,  1.57226562e-01,\n",
       "        5.83496094e-02, -3.58886719e-02,  2.12890625e-01, -1.13769531e-01,\n",
       "        1.41601562e-01, -1.29394531e-02,  9.13085938e-02, -3.95507812e-02,\n",
       "        9.76562500e-02, -2.69775391e-02,  1.30004883e-02, -1.30859375e-01,\n",
       "        3.32031250e-01, -3.53515625e-01, -5.44433594e-02, -2.50244141e-02,\n",
       "       -1.42578125e-01,  6.49414062e-02,  5.54199219e-02, -4.83398438e-02,\n",
       "       -1.12304688e-01, -1.32812500e-01, -6.73828125e-02, -1.41601562e-01,\n",
       "       -2.05078125e-01, -1.29882812e-01, -1.04003906e-01, -8.10546875e-02,\n",
       "       -1.67968750e-01,  1.63085938e-01, -1.13769531e-01, -5.17578125e-02,\n",
       "        7.61718750e-02,  3.59375000e-01,  1.04003906e-01,  3.59375000e-01,\n",
       "       -8.74023438e-02,  6.54296875e-02, -1.09863281e-02, -1.88476562e-01,\n",
       "       -6.59179688e-02,  2.30468750e-01, -2.96875000e-01,  6.59179688e-03,\n",
       "        1.49414062e-01, -1.73828125e-01,  1.31835938e-01,  2.36328125e-01,\n",
       "       -9.22851562e-02,  1.70898438e-01, -1.70898438e-02,  3.12500000e-02,\n",
       "       -3.37219238e-03,  9.66796875e-02, -2.61718750e-01, -1.84326172e-02,\n",
       "       -1.85546875e-01,  1.24023438e-01,  3.00781250e-01,  2.43164062e-01,\n",
       "        3.06640625e-01, -3.28125000e-01, -5.05371094e-02,  1.01562500e-01,\n",
       "        7.86132812e-02, -1.44531250e-01, -1.25976562e-01, -2.41699219e-02,\n",
       "        2.94921875e-01, -1.50390625e-01, -3.97949219e-02,  2.75390625e-01,\n",
       "        1.26953125e-01, -9.86328125e-02, -1.39648438e-01,  2.52685547e-02,\n",
       "       -8.54492188e-02, -1.72119141e-02,  9.17968750e-02,  1.39648438e-01,\n",
       "       -2.39257812e-01, -2.11914062e-01, -2.21679688e-01,  1.53320312e-01,\n",
       "       -1.58691406e-02, -2.00195312e-01, -2.07519531e-02,  3.58886719e-02,\n",
       "       -6.96629286e-07, -2.13867188e-01,  2.00195312e-01, -1.09375000e-01,\n",
       "       -5.15136719e-02,  6.22558594e-02, -3.22265625e-01, -7.86132812e-02,\n",
       "        5.02929688e-02,  7.08007812e-02,  1.20117188e-01, -1.79687500e-01,\n",
       "        1.59179688e-01, -1.02233887e-03, -3.49609375e-01,  1.25000000e-01,\n",
       "        6.44531250e-02,  8.10546875e-02, -3.39355469e-02,  7.42187500e-02,\n",
       "       -3.08837891e-02, -1.38671875e-01, -3.19824219e-02,  1.99218750e-01,\n",
       "        1.25000000e-01,  5.68847656e-02, -1.67968750e-01,  1.30859375e-01,\n",
       "        2.90527344e-02, -1.49536133e-02, -1.39648438e-01,  4.07714844e-02,\n",
       "       -1.05590820e-02, -1.74804688e-01,  2.12890625e-01, -1.41601562e-01,\n",
       "        2.30712891e-02, -3.36914062e-02, -8.78906250e-02, -6.64062500e-02,\n",
       "       -6.93359375e-02, -7.42187500e-02,  7.03125000e-02, -2.01416016e-02,\n",
       "       -1.26953125e-01, -3.63769531e-02,  5.93261719e-02,  1.18164062e-01,\n",
       "       -6.34765625e-03, -7.42187500e-02,  3.19824219e-02,  6.68945312e-02,\n",
       "       -2.27539062e-01,  6.54296875e-02,  1.79443359e-02,  1.46484375e-01,\n",
       "       -5.49316406e-02, -1.15234375e-01, -2.16796875e-01,  8.74023438e-02,\n",
       "        2.61718750e-01,  1.54296875e-01,  6.71386719e-03, -2.78320312e-02,\n",
       "       -4.15039062e-03, -2.09960938e-02, -5.51757812e-02, -9.76562500e-03,\n",
       "       -1.29882812e-01,  1.31835938e-01, -8.42285156e-03,  2.29492188e-01,\n",
       "        1.78710938e-01,  1.94335938e-01,  4.68750000e-02,  2.18505859e-02,\n",
       "       -2.75878906e-02,  1.73828125e-01,  1.33789062e-01,  1.36718750e-01,\n",
       "        3.10546875e-01,  9.39941406e-03,  9.22851562e-02, -2.44140625e-01,\n",
       "       -5.10253906e-02,  7.81250000e-02, -1.43554688e-01,  9.17968750e-02,\n",
       "        2.96630859e-02,  9.46044922e-03, -2.04101562e-01,  1.60156250e-01,\n",
       "        1.43554688e-01, -2.02636719e-02,  2.13623047e-02, -6.98242188e-02,\n",
       "       -3.11279297e-03, -2.52685547e-02, -1.09863281e-01,  1.07910156e-01,\n",
       "       -7.03125000e-02, -1.27929688e-01, -5.07812500e-02,  4.27246094e-02,\n",
       "       -7.32421875e-02, -3.54003906e-02,  8.88671875e-02, -3.02734375e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v['happy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('glad', 0.7408890724182129),\n",
       " ('pleased', 0.6632170677185059),\n",
       " ('ecstatic', 0.6626912355422974),\n",
       " ('overjoyed', 0.6599286794662476),\n",
       " ('thrilled', 0.6514049768447876),\n",
       " ('satisfied', 0.6437948942184448),\n",
       " ('proud', 0.636042058467865),\n",
       " ('delighted', 0.6272379159927368),\n",
       " ('disappointed', 0.6269949674606323),\n",
       " ('excited', 0.6247665882110596)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['happy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "happy는 glad, pleased와 유사한데 확실히 유사한 단어들인것을 느낄수있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('relatives', 0.6662652492523193),\n",
       " ('familiy', 0.6517066955566406),\n",
       " ('families', 0.6252894401550293),\n",
       " ('siblings', 0.6140849590301514),\n",
       " ('friends', 0.6128394603729248),\n",
       " ('mother', 0.6065611839294434),\n",
       " ('aunt', 0.5811319947242737),\n",
       " ('grandparents', 0.576207160949707),\n",
       " ('father', 0.5717043876647949),\n",
       " ('Family', 0.5672314763069153)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['family'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "family의 오타인 familiy도 보임. 오타도 비슷한 상황에서 나타나기 때문에 유사한 단어로 계산된게 볼수있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('elementary', 0.7868632674217224),\n",
       " ('schools', 0.7411909103393555),\n",
       " ('elementary_schools', 0.6597153544425964),\n",
       " ('kindergarten', 0.6529810428619385),\n",
       " ('eighth_grade', 0.6488089561462402),\n",
       " ('School', 0.6477997303009033),\n",
       " ('teacher', 0.63824063539505),\n",
       " ('students', 0.6301523447036743),\n",
       " ('classroom', 0.6281620860099792),\n",
       " ('Schools', 0.6172096133232117)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['school'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 WEAT를 통해 이 모델의 편향성을 확인할 수 있음.\n",
    "\n",
    "아래는 [논문](https://arxiv.org/pdf/1608.07187.pdf)에 있는 단어 SET으로 구성되어있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4821917"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_A = ['science', 'technology', 'physics', 'chemistry', 'Einstein', 'NASA', 'experiment', 'astronomy']\n",
    "target_B = ['poetry', 'art', 'Shakespeare', 'dance', 'literature', 'novel', 'symphony', 'drama']\n",
    "attribute_X = ['brother', 'father', 'uncle', 'grandfather', 'son', 'he', 'his', 'him']\n",
    "attribute_Y = ['sister', 'mother', 'aunt', 'grandmother', 'daughter', 'she', 'hers', 'her']\n",
    "\n",
    "A = np.array([w2v[word] for word in target_A])\n",
    "B = np.array([w2v[word] for word in target_B])\n",
    "X = np.array([w2v[word] for word in attribute_X])\n",
    "Y = np.array([w2v[word] for word in attribute_Y])\n",
    "\n",
    "weat_score(X, Y, A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "과학 관련 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6929383"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_A = ['pizza', 'coke', 'hamburger', 'ham', 'ramen', 'icecream', 'candy']\n",
    "target_B = ['salad', 'fruit', 'vegetable', 'herb', 'root', 'greens', 'wholesome']\n",
    "attribute_X = ['junk', 'canned', 'convenience', 'frozen', 'fast']\n",
    "attribute_Y = ['health', 'beneficial', 'good', 'nourishing', 'nutritious']\n",
    "\n",
    "A = np.array([w2v[word] for word in target_A])\n",
    "B = np.array([w2v[word] for word in target_B])\n",
    "X = np.array([w2v[word] for word in attribute_X])\n",
    "Y = np.array([w2v[word] for word in attribute_Y])\n",
    "\n",
    "weat_score(X, Y, A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.082050726"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_A = ['pizza', 'coke', 'hamburger', 'ham', 'ramen', 'icecream', 'candy']\n",
    "target_B = ['salad', 'fruit', 'vegetable', 'herb', 'root', 'greens', 'wholesome']\n",
    "attribute_X = ['book', 'essay', 'dictionary', 'magazine', 'novel']\n",
    "attribute_Y = ['news', 'report', 'statement', 'broadcast', 'word']\n",
    "\n",
    "A = np.array([w2v[word] for word in target_A])\n",
    "B = np.array([w2v[word] for word in target_B])\n",
    "X = np.array([w2v[word] for word in attribute_X])\n",
    "Y = np.array([w2v[word] for word in attribute_Y])\n",
    "\n",
    "weat_score(X, Y, A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "즉, 임베딩 모델이 판단하기에 어느 것끼리 가깝다고 말할 수 없는 것이지요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삭제완료\n"
     ]
    }
   ],
   "source": [
    "del w2v\n",
    "print('삭제완료')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 직접만드는 Word Embedding에 WEAT 적용(1)\n",
    "\n",
    "지금까지는 제시된 모델과 단어들로 WEAT score를 구해보았습니다.\n",
    "이제 주어진 데이터로 다음과 같은 과정을 수행해보도록 하겠습니다.\n",
    "\n",
    "1. 형태소 분석기를 이용하여 품사가 명사인 경우, 해당 단어를 추출하기\n",
    "\n",
    "2. 추출된 결과로 embedding model 만들기\n",
    "\n",
    "3. TF/IDF로 해당 데이터를 가장 잘 표현하는 단어 셋 만들기\n",
    "\n",
    "4. embedding model과 단어 셋으로 WEAT score 구해보기\n",
    "\n",
    "####  1. 형태소 분석기를 이용하여 품사가 명사인 경우 해당 단어를 추출하기\n",
    "synopsis.txt(대략 17MB)에는 2001년부터 2019년 8월까지 제작된 영화들의 시놉시스 정보가 있습니다.\n",
    "(개봉된 영화 중 일부만 포함되어있습니다. 더 많은 영화 정보를 원하시면 KOBIS에서 확인하시기 바랍니다.)\n",
    "synopsis.txt의 일부를 읽어볼까요?\n",
    "\n",
    "\n",
    "``` terminal\n",
    "$ wget https://aiffelstaticprd.blob.core.windows.net/media/documents/synopsis.zip\n",
    "$ mv synopsis.zip ~/aiffel/weat\n",
    "$ cd ~/aiffel/weat && unzip synopsis.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사운드 엔지니어 상우(유지태 분)는 치매에 걸린 할머니(백성희 분)와\n",
      " 젊은 시절 상처한 한 아버지(박인환 분), 고모(신신애 분)와 함께 살고 있다.\n",
      " 어느 겨울 그는 지방 방송국 라디오 PD 은수(이영애 분)를 만난다.\n",
      " 자연의 소리를 채집해 틀어주는 라디오 프로그램을 준비하는 은수는 상우와 녹음 여행을 떠난다.\n",
      " 자연스레 가까워지는 두 사람은 어느 날, 은수의 아파트에서 밤을 보낸다.\n",
      " 너무 쉽게 사랑에 빠진 두 사람... 상우는 주체할 수 없을 정도로 그녀에게 빨려든다.\n",
      " 그러나 겨울에 만난 두 사람의 관계는 봄을 지나 여름을 맞이하면서 삐걱거린다.\n",
      " 이혼 경험이 있는 은수는 상우에게 결혼할 생각이 없다며 부담스러운 표정을 내비친다.\n",
      " \"어떻게 사랑이 변하니?...\"라고 묻는 상우에게 은수는 그저 \"헤어져\" 라고 단호하게 말한다.\n",
      " 영원히 변할 것 같지 않던 사랑이 변하고, 그 사실을 받아들이지 못하는 상우는 어찌 할 바를 모른다.\n",
      " 은수를 잊지 못하는 상우는 미련과 집착의 감정을 이기지 못하고 서울과 강릉을 오간다.\n",
      "유사 이래 연령, 성별, 빈부의 차이와 정치적인 입장을 불문하고 일거에 국민을 통합해 온 '애국심'이라는 성역에 일침을 가하는 다큐멘터리. 재작년 전국 민족민주 유가족협의회의 장기농성을 다룬 인상적인 다큐멘터리 <민들레>를 만들었던 독립영화집단 '빨간 눈사람'이 우리 사회 구석구석을 발빠르게 돌아다니며 애국심과 민족주의가 강요되는 현장을 발굴하여 카메라에 담았다. 박홍 서강대 명예총장, 이도형 '한국논단' 발행인, 축구해설자 신문선, 홍세화, 박노해 등 사회 각계의 '스타'들이 등장해 저마다의 확고한 신념을 성토한다. 감독 이경순과 최하동하는 이 작품을 위해 3년간 백여 명을 인터뷰했다고 한다. 2001 올해의 독립영화상 수상.\n",
      " 민족과 국가란 공동체에서 부단히 권력과 부를 얻는 자, 나아가 민족과 국가란 공동체에서 얻은 신분과 부귀를 영원히 그의 자손에게 대물림하려는 자, 그래서 민족과 국가란 공동체를 부단히 유지해야만 하는 자, 따라서 민족과 국가란 공동체의 당위성과 개인의 가치를 초월하는 그 존엄성을 끝도 없이 창조하고 되뇌어야 하는 자, 종국에는 민족과 국가란 공동체에 속해 있다고 태내에서부터 세뇌된 모든 이들의 삶과 행동에서 영원히 자기복제되는 순환의 고리, 영생하는 애국의 원동력은 그 순환의 골에서 온다.\n",
      "엽기적인 살인사건이 발생한 장소를 관광하는 투어팀. 그 팀에서 관광객들은 살인사건과 관련하여 히스테리컬한 반응을 보이는데 과연 이들의 정체는? (Tourists see whrer a murder take place. They respond hysterically to the murder…what are they?)\n",
      " 제46회 발라돌리드 국제영화제 (2001, 스페인)\n",
      "착하지만 엉뚱한 태희(배두나 분), 예쁜 깍쟁이 혜주(이요원 분), 그림을 잘 그리는 지영(옥지영 분), 명랑한 쌍둥이 비류(이은실 분)와 온조(이은주 분)는 단짝친구들. 늘 함께였던 그들이지만 스무 살이 되면서 길이 달라진다. 증권회사에 입사한 혜주는 성공한 커리어우먼의 야심을 키우고 미술에 재능이 있는 지영은 유학을 꿈꾼다. 한편 태희는 봉사활동에서 알게 된 뇌성마비 시인을 좋아하는데...\n",
      "  어느 날 지영이 길 잃은 새끼 고양이 티티를 만남면서 스무 살 그녀들의 삶에 고양이 한 마리가 끼어들게 된다. 혼자 있길 좋아하고, 쉽게 마음을 열지 않는 신비로운 동물 고양이. 고양이를 닮은 스무 살 그녀들. 고양이 티티와 함께 한 시간동안 삶은 예상못한 방향으로 흘러가지만 마침내 그녀들만의 해결책을 찾게 되는데... 사랑스런 몽상가 태희, 아름다운 야심가 혜주, 신비로운 아웃사이더 지영. 마지막으로 고양이를 부탁받은 사람은 누구일까?\n",
      "인도 등 아시아 식민지에 처음 발을 디딘 뒤 여행하고 “경영”한 이들은 과연 누구였을까? 과거의 이미지들은, 이민과 인종 문제, ‘오리엔탈리즘’이 격렬히 충돌하고 있는 현재와 강력하게 공명한다.\n",
      " [제19회 인디다큐페스티발]\n",
      "홀로 살아가는 미국 할머니와 한국 할머니의 이야기. 공원에서 가끔 마주치게 되는 그들은 비록 언어 소통의 어려움을 겪지만 시간이 흘러감에 따라 서로 가까워져 그들의 외로움과 우정을 공유하게 된다. 겨울이 지나고 봄이 왔을 때 길가의 민들레 홀씨는 삶의 이치를 말해주듯 한 할머니의 주위를 맴돈다. (Two elderly widows, an American and a Korean, frequent the same park in Philadelphia and attempt a friendship, though the Korean widow speaks no English. Driven by loneliness and a spark of hope, they persevere within the limits of body language, and the outcome poses a question of life as fundamental as a flower.)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "with open(os.getenv('HOME')+'/aiffel/weat/synopsis.txt', 'r') as file:\n",
    "    for i in range(20):\n",
    "        print(file.readline(), end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 synopsis.txt 파일을 읽어 품사가 명사인 경우만 남겨 tokenized라는 변수명으로 저장해봅시다.\n",
    "konlpy 패키지를 이용해봅시다.\n",
    "\n",
    "``` terminal\n",
    "$ pip install konlpy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 약 15분정도 걸립니다.\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "tokenized = []\n",
    "with open(os.getenv('HOME')+'/aiffel/weat/synopsis.txt', 'r') as file:\n",
    "    while True:\n",
    "        line = file.readline()\n",
    "        if not line: break\n",
    "        words = okt.pos(line, stem=True, norm=True)\n",
    "        res = []\n",
    "        for w in words:\n",
    "            if w[1] in [\"Noun\"]:      # \"Adjective\", \"Verb\" 등을 포함할 수도 있습니다.\n",
    "                res.append(w[0])    # 명사일 때만 tokenized 에 저장하게 됩니다. \n",
    "        tokenized.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71156\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 추출된 결과로 embedding model 만들기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssac4/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('작품', 0.9070759415626526),\n",
       " ('다큐멘터리', 0.8515136241912842),\n",
       " ('드라마', 0.8352524042129517),\n",
       " ('영화로', 0.8190748691558838),\n",
       " ('형식', 0.8113033175468445),\n",
       " ('주제', 0.8026663064956665),\n",
       " ('소재', 0.7932949066162109),\n",
       " ('송일곤', 0.7918614149093628),\n",
       " ('스토리', 0.7907422780990601),\n",
       " ('코미디', 0.7901086211204529)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# tokenized에 담긴 데이터를 가지고 나만의 Word2Vec을 생성합니다. (Gensim 4.0 기준)\n",
    "# model = Word2Vec(tokenized, vector_size=100, window=5, min_count=3, sg=0)  \n",
    "# model.wv.most_similar(positive=['영화'])\n",
    "\n",
    "# Gensim 3.X 에서는 아래와 같이 생성합니다. \n",
    "model = Word2Vec(tokenized, size=100, window=5, min_count=3, sg=0)  \n",
    "model.most_similar(positive=['영화'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어떤가요? 나만의 Word2Vec이 잘 훈련된 거 같나요? 아래와 같이 좀더 확인해 봅시다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('애정', 0.7341066598892212),\n",
       " ('연애', 0.7116760611534119),\n",
       " ('행복', 0.70481276512146),\n",
       " ('첫사랑', 0.7040883302688599),\n",
       " ('시빌라', 0.7036919593811035),\n",
       " ('아르튬', 0.7003369331359863),\n",
       " ('진심', 0.7002869844436646),\n",
       " ('만남', 0.6879246234893799),\n",
       " ('우정', 0.6843553781509399),\n",
       " ('열정', 0.6804129481315613)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['사랑'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('영화감독', 0.9031145572662354),\n",
       " ('배우', 0.8892683982849121),\n",
       " ('영감', 0.8834878206253052),\n",
       " ('대본', 0.8815126419067383),\n",
       " ('시나리오', 0.8780593872070312),\n",
       " ('연기자', 0.8775286674499512),\n",
       " ('각색', 0.8770350217819214),\n",
       " ('캐스팅', 0.8764846324920654),\n",
       " ('공연', 0.8761006593704224),\n",
       " ('원표', 0.8753910064697266)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['연극'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-7. 직접 만드는 Word Embedding에 WEAT 적용(2)\n",
    "### 3. TF-IDF로 해당 데이터를 가장 잘 표현하는 단어 셋 만들기\n",
    "-----------------------\n",
    "WEAT score를 구할 때 단어 셋을 만들어주어야 합니다.\n",
    "targets_X, targets_Y, attribute_A, attribute_B를 만들어주었던 것이 기억나시죠?\n",
    "그렇다면 우리는 두 축을 어떤 기준으로 잡고, 해당 축의 어떤 항목을 사용할지 정해야 합니다. 여기서는 두 축을 영화 장르, 영화 구분 정보를 이용하겠습니다. (영화 구분 정보란 일반영화, 예술영화, 독립영화로 구분된 정보입니다. KOBIS에서 제공한 정보를 기준으로 분류하였습니다. )\n",
    "\n",
    "\n",
    "* 영화 구분            \n",
    "    - synopsis_art.txt : 예술영화         \n",
    "    - synopsis_gen.txt : 일반영화(상업영화)                \n",
    "    - 그 외 독립영화 등으로 분류됩니다.                   \n",
    "\n",
    "* 장르 구분       \n",
    "    - synopsis_SF.txt: SF                \n",
    "    - synopsis_가족.txt: 가족\n",
    "    - synopsis_공연.txt: 공연\n",
    "    - synopsis_공포(호러).txt: 공포(호러)\n",
    "    - synopsis_기타.txt: 기타\n",
    "    - synopsis_다큐멘터리.txt: 다큐멘터리\n",
    "    - synopsis_드라마.txt: 드라마\n",
    "    - synopsis_멜로로맨스.txt: 멜로로맨스\n",
    "    - synopsis_뮤지컬.txt: 뮤지컬\n",
    "    - synopsis_미스터리.txt: 미스터리\n",
    "    - synopsis_범죄.txt: 범죄\n",
    "    - synopsis_사극.txt: 사극\n",
    "    - synopsis_서부극(웨스턴).txt: 서부극(웨스턴)\n",
    "    - synopsis_성인물(에로).txt: 성인물(에로)\n",
    "    - synopsis_스릴러.txt: 스릴러\n",
    "    - synopsis_애니메이션.txt: 애니메이션\n",
    "    - synopsis_액션.txt: 액션\n",
    "    - synopsis_어드벤처.txt: 어드벤처\n",
    "    - synopsis_전쟁.txt: 전쟁\n",
    "    - synopsis_코미디.txt: 코미디\n",
    "    - synopsis_판타지.txt: 판타지\n",
    "\n",
    "이번에는 예술영화와 일반영화(상업영화)라는 영화구분을 target으로 삼고, 드라마 장르와 액션 장르라는 장르구분을 attribute로 삼아 WEAT score를 계산해 보겠습니다.\n",
    "이것의 의미는, 드라마 장르에는 예술영화적 성격이 강하고, 액션 장르에는 일반(상업)영화적 성격이 강할 것이라는 편향성이 워드 임베딩 상에 얼마나 나타나고 있는지를 측정해 보겠다는 것입니다.\n",
    "\n",
    "'synopsis_art.txt', 'synopsis_gen.txt' 두 파일을 읽고, 위에서 했던 것과 마찬가지로 명사에 대해서만 추출하여 art, gen 변수에 할당하시기 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "art_txt = 'synopsis_art.txt'\n",
    "gen_txt = 'synopsis_gen.txt'\n",
    "\n",
    "def read_token(file_name):\n",
    "    okt = Okt()\n",
    "    result = []\n",
    "    with open(os.getenv('HOME')+'/aiffel/weat/'+file_name, 'r') as fread: \n",
    "        print(file_name, '파일을 읽고 있습니다.')\n",
    "        while True:\n",
    "            line = fread.readline() \n",
    "            if not line: break \n",
    "            tokenlist = okt.pos(line, stem=True, norm=True) \n",
    "            for word in tokenlist:\n",
    "                if word[1] in [\"Noun\"]:#, \"Adjective\", \"Verb\"]:\n",
    "                    result.append((word[0])) \n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synopsis_art.txt 파일을 읽고 있습니다.\n",
      "synopsis_gen.txt 파일을 읽고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# 2개의 파일을 처리하는데 10분 가량 걸립니다. \n",
    "art = read_token(art_txt)\n",
    "gen = read_token(gen_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WEAT 계산을 위해서는 총 4개의 단어 셋 X, Y, A, B가 필요합니다. 예를 들어, 예술영화라는 개념을 가장 잘 대표하는 단어들을 art_txt를 처리해서 만든 art라는 단어 리스트에서부터 골라내야 하는 것입니다. 이를 위해서 우리가 각자의 상식을 동원해서 적절한 단어를 골라낼 수도 있을 것입니다. 하지만 보다 납득할 수 있는 보편적인 방법을 사용하기를 바랍니다.\n",
    "\n",
    "어떤 개념을 나타내는 단어를 선정하는 방법으로 어떤 것이 적당할까요? 꼭 정해진 방법이 있는 것은 아닙니다. 그러나 이번 경우에는 예술영화, 일반영화라는 영화구분별로 시놉시스를 모아 데이터를 구성했습니다. 그렇다면 예술영화를 잘 대표하는 단어란, 예술영화 시놉시스에는 자주 나타나지만 그 외 다른 구분의 영화(예를 들어 일반영화) 시놉시스에는 자주 나타나지 않는 것을 고르는 것이 적당할 것입니다.\n",
    "\n",
    "이런 것과 비슷한 개념의 단어 분석 방식 중 TF-IDF라는 것을 이미 접해 보셨을 것입니다. 즉, 코퍼스에서 자주 나타나는(TF가 높은) 단어이지만, 다른 코퍼스에까지 두루 걸쳐 나오지는 않는(IDF가 높은) 단어를 선정하고 싶은 것입니다.\n",
    "이번에는 단어 셋 구성을 위해 TF-IDF방식을 사용하겠습니다. (그러나 이 방식이 최선이라는 것은 아닙니다.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 41082)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform([art, gen])\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23976\n",
      "영화\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_['영화'])\n",
    "print(vectorizer.get_feature_names()[23976])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예술영화를 대표하는 단어들:\n",
      "그녀, 자신, 시작, 위해, 사랑, 사람, 영화, 친구, 남자, 가족, 이야기, 마을, 사건, 마음, 세상, 아버지, 아이, 엄마, 모든, 여자, 대한, 서로, 과연, 다시, 시간, 아들, 소녀, 아내, 다른, 사이, 영화제, 세계, 사실, 하나, 점점, 남편, 감독, 여행, 인생, 발견, 모두, 순간, 우리, 가장, 마지막, 생활, 아빠, 모습, 통해, 죽음, 기억, 비밀, 학교, 음악, 한편, 소년, 생각, 도시, 명의, 사고, 결혼, 전쟁, 때문, 위기, 이제, 최고, 이자, 과거, 일상, 경찰, 상황, 간다, 미국, 결심, 운명, 현실, 관계, 지금, 단편, 여인, 하루, 이름, 이후, 준비, 인간, 감정, 만난, 국제, 처음, 충격, 살인, 누구, 동안, 존재, 그린, 어머니, 연인, 계속, 동생, 작품, \n",
      "\n",
      "일반영화를 대표하는 단어들:\n",
      "자신, 그녀, 영화제, 위해, 사람, 시작, 국제, 영화, 친구, 사랑, 남자, 이야기, 대한, 서울, 여자, 사건, 남편, 아이, 가족, 아버지, 다른, 마을, 시간, 엄마, 아들, 모든, 단편, 마음, 사실, 다시, 세계, 모습, 작품, 통해, 생각, 서로, 세상, 발견, 감독, 아내, 관계, 소녀, 사이, 하나, 우리, 애니메이션, 때문, 여성, 죽음, 과연, 점점, 인간, 생활, 한편, 결혼, 상황, 모두, 기억, 명의, 소년, 여행, 가장, 간다, 순간, 이제, 도시, 비밀, 학교, 과거, 가지, 이자, 경찰, 마지막, 미국, 동안, 전쟁, 주인공, 대해, 존재, 현실, 연출, 사고, 살인, 일상, 어머니, 계속, 사회, 인생, 다큐멘터리, 부문, 섹스, 최고, 바로, 동생, 의도, 하루, 위기, 계획, 정체, 한국, "
     ]
    }
   ],
   "source": [
    "m1 = X[0].tocoo()   # art를 TF-IDF로 표현한 sparse matrix를 가져옵니다. \n",
    "m2 = X[1].tocoo()   # gen을 TF-IDF로 표현한 sparse matrix를 가져옵니다. \n",
    "\n",
    "w1 = [[i, j] for i, j in zip(m1.col, m1.data)]\n",
    "w2 = [[i, j] for i, j in zip(m2.col, m2.data)]\n",
    "\n",
    "w1.sort(key=lambda x: x[1], reverse=True)   #art를 구성하는 단어들을 TF-IDF가 높은 순으로 정렬합니다. \n",
    "w2.sort(key=lambda x: x[1], reverse=True)   #gen을 구성하는 단어들을 TF-IDF가 높은 순으로 정렬합니다. \n",
    "\n",
    "print('예술영화를 대표하는 단어들:')\n",
    "for i in range(100):\n",
    "    print(vectorizer.get_feature_names()[w1[i][0]], end=', ')\n",
    "\n",
    "print('\\n')\n",
    "    \n",
    "print('일반영화를 대표하는 단어들:')\n",
    "for i in range(100):\n",
    "    print(vectorizer.get_feature_names()[w2[i][0]], end=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어떤가요? 두 개념을 대표하는 단어를 TF-IDF가 높은 순으로 추출하고 싶었는데, 양쪽에 중복된 단어가 너무 많은 것을 볼 수 있습니다.\n",
    "두 개념축이 대조되도록 대표하는 단어 셋을 만들고 싶기 때문에 중복되지 않게 추출하도록 합니다.\n",
    "우선 상위 100개의 단어들 중 중복되는 단어를 제외하고 상위 n(=15)개의 단어를 추출합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 15\n",
    "w1_, w2_ = [], []\n",
    "for i in range(100):\n",
    "    w1_.append(vectorizer.get_feature_names()[w1[i][0]])\n",
    "    w2_.append(vectorizer.get_feature_names()[w2[i][0]])\n",
    "\n",
    "# w1에만 있고 w2에는 없는, 예술영화를 잘 대표하는 단어를 15개 추출한다.\n",
    "target_art, target_gen = [], []\n",
    "for i in range(100):\n",
    "    if (w1_[i] not in w2_) and (w1_[i] in model.wv): target_art.append(w1_[i])\n",
    "    if len(target_art) == n: break \n",
    "\n",
    "# w2에만 있고 w1에는 없는, 일반영화를 잘 대표하는 단어를 15개 추출한다.\n",
    "for i in range(100):\n",
    "    if (w2_[i] not in w1_) and (w2_[i] in model.wv): target_gen.append(w2_[i])\n",
    "    if len(target_gen) == n: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추출된 단어를 살펴볼까요?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아빠', '음악', '결심', '운명', '지금', '여인', '이름', '이후', '준비', '감정', '만난', '처음', '충격', '누구', '그린']\n"
     ]
    }
   ],
   "source": [
    "print(target_art)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['서울', '애니메이션', '여성', '가지', '주인공', '대해', '연출', '사회', '다큐멘터리', '부문', '섹스', '바로', '의도', '계획', '정체']\n"
     ]
    }
   ],
   "source": [
    "print(target_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 장르별 대표 단어를 추출해 봅시다. 이번에는 드라마 장르와 액션 장르를 다루어 보려고 합니다. 그러나 그렇다고 해서 드라마와 액션 단 2개의 장르만 고려하기보다는 여러 장르의 코퍼스를 두루 고려하는 것이 특정 장르를 대표하는 단어를 선택하는 데 더 유리할 것입니다. 이번에는 주요 장르 5개만 고려해 봅시다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_txt = ['synopsis_drama.txt', 'synopsis_romance.txt', 'synopsis_action.txt', 'synopsis_comedy.txt', 'synopsis_war.txt', 'synopsis_horror.txt']\n",
    "genre_name = ['드라마', '멜로로맨스', '액션', '코미디', '전쟁', '공포(호러)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synopsis_drama.txt 파일을 읽고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# 약 10분정도 걸립니다.\n",
    "genre = []\n",
    "for file_name in genre_txt:\n",
    "    genre.append(read_token(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(genre)\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = [X[i].tocoo() for i in range(X.shape[0])]\n",
    "\n",
    "w = [[[i, j] for i, j in zip(mm.col, mm.data)] for mm in m]\n",
    "\n",
    "for i in range(len(w)):\n",
    "    w[i].sort(key=lambda x: x[1], reverse=True)\n",
    "attributes = []\n",
    "for i in range(len(w)):\n",
    "    print(genre_name[i], end=': ')\n",
    "    attr = []\n",
    "    j = 0\n",
    "    while (len(attr) < 15):\n",
    "        if vectorizer.get_feature_names()[w[i][j][0]] in model.wv:\n",
    "            attr.append(vectorizer.get_feature_names()[w[i][j][0]])\n",
    "            print(vectorizer.get_feature_names()[w[i][j][0]], end=', ')\n",
    "        j += 1\n",
    "    attributes.append(attr)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 장르를 대표하는 단어들을 추출해보았습니다. 우리가 생각했던 직관과 데이터에 있는 단어들이 잘 맞나요?\n",
    "중복된 것이 종종 있지만 art, gen 두 개의 단어 셋을 추출했을 때에 비해 적습니다. 그러므로 중복을 체크해서 삭제하기보다 그대로 사용하겠습니다.\n",
    "\n",
    "## 4. embedding model과 단어 셋으로 WEAT score 구해보기\n",
    "이제 WEAT_score를 구해봅시다.\n",
    "traget_A는 art, target_B는 gen, attribute_X는 '드라마', attribute_Y는 '액션' 과 같이 정해줄 수 있습니다.\n",
    "\n",
    "target_A 는 art, target_B 는 gen으로 고정하고 attribute_X, attribute_Y를 바꿔가면서 구해봅시다.\n",
    "구한 결과를 21x21 매트릭스 형태로 표현해서 matrix 라는 변수에 담아봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = [[0 for _ in range(len(genre_name))] for _ in range(len(genre_name))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([model.wv[word] for word in target_art])\n",
    "B = np.array([model.wv[word] for word in target_gen])\n",
    "\n",
    "for i in range(len(genre_name)-1):\n",
    "    for j in range(i+1, len(genre_name)):\n",
    "        X = np.array([model.wv[word] for word in attributes[i]])\n",
    "        Y = np.array([model.wv[word] for word in attributes[j]])\n",
    "        matrix[i][j] = weat_score(X, Y, A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "matrix를 채워보았습니다.\n",
    "WEAT score 값이 2와 -2에 가까운 수치들을 보고, 과연 우리의 직관과 비슷한지 살펴볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(genre_name)-1):\n",
    "    for j in range(i+1, len(genre_name)):\n",
    "        if matrix[i][j] > 1.1 or matrix[i][j] < -1.1:\n",
    "            print(genre_name[i], genre_name[j],matrix[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 예술영화와 일반영화, 그리고 다큐멘터리와 멜로로맨스의 WEAT score의 의미를 해석해보면 예술 영화는 멜로로맨스와 가깝고, 다큐멘터리는 일반 영화와 가깝다고 볼 수 있습니다.\n",
    "* 예술영화와 일반영화, 그리고 멜로로맨스와 전쟁의 WEAT score의 의미를 해석해보면 예술 영화는 멜로로맨스와 가깝고, 전쟁은 일반 영화와 가깝다고 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np; \n",
    "import seaborn as sns; \n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# 한글 지원 폰트\n",
    "sns.set(font=\"Noto Sans CJK JP\")\n",
    "\n",
    "ax = sns.heatmap(matrix, xticklabels=genre_name, yticklabels=genre_name, annot=True,  cmap='RdYlGn_r')\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6-8. 프로젝트 : 모든 장르 간 편향성 측정해 보기\n",
    "지금까지 우리는 영화 시놉시스 코퍼스를 가지고 영화 구분과 영화 장르 간에 내재된 편향성을 측정하는 작업을 진행해 보았습니다. 어느 정도는 우리의 상식과 일치하는 편향성이 측정되었을 것입니다.\n",
    "\n",
    "이번에는 모든 장르에 대해 영화 구분과의 편향성 정도를 측정해 보겠습니다. 대부분의 과정은 이전 스텝에서 이미 진행한 내용을 참고해서 동일하게 진행 가능할 것입니다.\n",
    "\n",
    "STEP 1. 형태소 분석기를 이용하여 품사가 명사인 경우 해당 단어를 추출하기\n",
    "STEP 2. 추출된 결과로 embedding model 만들기\n",
    "STEP 3. target, attribute 단어 셋 만들기\n",
    "이전 스텝에서는 TF-IDF를 사용해서 단어 셋을 만들었습니다. 이 방법으로도 어느 정도는 대표 단어를 잘 선정할 수 있습니다. 그러나 TF-IDF가 높은 단어를 골랐음에도 불구하고 중복되는 단어가 발생하는 문제가 있었습니다.\n",
    "개념축을 표현하는 단어가 제대로 선정되지 않은 것은 WEAT 계산 결과에 악영향을 미칩니다.\n",
    "\n",
    "혹시 TF-IDF를 적용했을 때의 문제점이 무엇인지 지적 가능하다면 그 문제점을 지적하고 스스로 방법을 개선하여 대표 단어 셋을 구축해 보기 바랍니다. TF-IDF 방식을 쓰더라도 중복된 단어를 잘 제거하면 여전히 유용한 방식이 될 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
