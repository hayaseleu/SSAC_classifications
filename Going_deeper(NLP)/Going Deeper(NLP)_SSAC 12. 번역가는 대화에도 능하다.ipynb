{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12-1 들어가면서\n",
    "\n",
    "학습목표\n",
    "\n",
    "1. 번역 성능을 측정하는 BLUE score와 Beam Search Decoder를 익히고\n",
    "\n",
    "2. chatbot을 만들어보기 \n",
    "\n",
    "파일 경로\n",
    "\n",
    "``` teriminal\n",
    "$ mkdir -p ~/aiffel/transformer_chatbot\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Ubuntu KoNLPy 설치\n",
    "\n",
    "``` terminal\n",
    "$ sudo apt-get install g++ openjdk-8-jdk\n",
    "$ sudo apt-get install curl\n",
    "$ bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
    "$ pip install konlpy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## library와 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from konlpy.tag import Mecab\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data 다운로드\n",
    "\n",
    "[데이터 링크](https://github.com/songys/Chatbot_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"ChatbotData .csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>티가 나니까 눈치가 보이는 거죠!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>훔쳐보는 거 티나나봐요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>흑기사 해주는 짝남.</td>\n",
       "      <td>설렜겠어요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까?</td>\n",
       "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>도피성 결혼은 하지 않길 바라요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11823 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Q                         A  label\n",
       "0                       12시 땡!                하루가 또 가네요.      0\n",
       "1                  1지망 학교 떨어졌어                 위로해 드립니다.      0\n",
       "2                 3박4일 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "3              3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "4                      PPL 심하네                눈살이 찌푸려지죠.      0\n",
       "...                        ...                       ...    ...\n",
       "11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!      2\n",
       "11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.      2\n",
       "11820              흑기사 해주는 짝남.                    설렜겠어요.      2\n",
       "11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n",
       "11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2\n",
       "\n",
       "[11823 rows x 3 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q,A label로 이루어져있는데 0은 일상다반서, 이별(부정)은 1, 사랑(긍정)은 2로 labeling 되어있음. \n",
    "우리에게는 필요없는 정보라 제외함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = data.Q\n",
    "ansewers = data.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         12시 땡!\n",
       "1                    1지망 학교 떨어졌어\n",
       "2                   3박4일 놀러가고 싶다\n",
       "3                3박4일 정도 놀러가고 싶다\n",
       "4                        PPL 심하네\n",
       "                  ...           \n",
       "11818             훔쳐보는 것도 눈치 보임.\n",
       "11819             훔쳐보는 것도 눈치 보임.\n",
       "11820                흑기사 해주는 짝남.\n",
       "11821    힘든 연애 좋은 연애라는게 무슨 차이일까?\n",
       "11822                 힘들어서 결혼할까봐\n",
       "Name: Q, Length: 11823, dtype: object"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                      하루가 또 가네요.\n",
       "1                       위로해 드립니다.\n",
       "2                     여행은 언제나 좋죠.\n",
       "3                     여행은 언제나 좋죠.\n",
       "4                      눈살이 찌푸려지죠.\n",
       "                   ...           \n",
       "11818          티가 나니까 눈치가 보이는 거죠!\n",
       "11819               훔쳐보는 거 티나나봐요.\n",
       "11820                      설렜겠어요.\n",
       "11821    잘 헤어질 수 있는 사이 여부인 거 같아요.\n",
       "11822          도피성 결혼은 하지 않길 바라요.\n",
       "Name: A, Length: 11823, dtype: object"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ansewers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is sample sentence.\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "  \n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z0-9ㄱ-ㅎ|가-힣?.!,]+\", \" \", sentence)  # a-zA-Z0-9ㄱ-ㅎ|가-힣?.! 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This is sample # $ %       sentence.\"))   # 이 문장이 어떻게 필터링되는지 되는것을 통해 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus(s_s,t_s, length = 25):\n",
    "    que_corpus = [] \n",
    "    ans_corpus = []\n",
    "    try:\n",
    "        if len(s_s) != len(t_s):\n",
    "            raise Exception('소스 문장 데이터와 타켓 문장 데이터의 길이가 다릅니다.')\n",
    "    except Exception as e:                             # 예외가 발생했을 때 실행됨\n",
    "        print(f'소스 문장 데이터의 길이 {len(s_s)} 타켓 문장 데이터의 길이 {len(t_s)} 입니다.', e)\n",
    "        \n",
    "    for i in range(len(s_s)):\n",
    "        mecab = Mecab()\n",
    "        q, a = s_s[i], t_s[i]\n",
    "        q = preprocess_sentence(q)\n",
    "        q = mecab.morphs(q)\n",
    "        \n",
    "        a = preprocess_sentence(a)\n",
    "        a = mecab.morphs(a)\n",
    "        \n",
    "        if (len(q) < length) & (len(a) < length) : # 길이 제외\n",
    "            if (q not in que_corpus) & (a not in ans_corpus) : \n",
    "                que_corpus.append(q)\n",
    "                ans_corpus.append(a)\n",
    "    return que_corpus, ans_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수를 짤때 문장의 길이가 다르면 에러 처리를 해주게 만들어보았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "소스 문장 데이터의 길이 1 타켓 문장 데이터의 길이 11823 입니다. 소스 문장 데이터와 타켓 문장 데이터의 길이가 다릅니다.\n"
     ]
    }
   ],
   "source": [
    "que_corpus, ans_corpus = build_corpus(ansewers[:1],questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "que_corpus, ans_corpus = build_corpus(questions,ansewers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['12', '시', '땡', '!'],\n",
       " ['1', '지망', '학교', '떨어졌', '어'],\n",
       " ['3', '박', '4', '일', '놀', '러', '가', '고', '싶', '다'],\n",
       " ['ppl', '심하', '네'],\n",
       " ['sd', '카드', '망가졌', '어']]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "que_corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['하루', '가', '또', '가', '네요', '.'],\n",
       " ['위로', '해', '드립니다', '.'],\n",
       " ['여행', '은', '언제나', '좋', '죠', '.'],\n",
       " ['눈살', '이', '찌푸려', '지', '죠', '.'],\n",
       " ['다시', '새로', '사', '는', '게', '마음', '편해요', '.']]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_corpus[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation\n",
    "\n",
    "``` terminal\n",
    "$ pip install gensim\n",
    "```\n",
    "\n",
    "\n",
    "CV에만 있는줄알았는데 NLP에도 존재한다. 데이터가 1만개 정도 됨으로 augmentation 기법        \n",
    "그중에서도 **Embdedding을 활용한 Lexical Subsitution**을 구현해보도록 한다.       \n",
    "\n",
    "아래 링크에서 한국어로 사전 훈련된 Embedding 모델을 다운로드한다.\n",
    "Korean(w)가 Word2Vec으로 학습한 모델이며      \n",
    "용량이 적당함으로 다운로드해서 ko.bin 파일을 얻는다.    \n",
    "\n",
    "[Kyubyong/wordvectors](https://github.com/Kyubyong/wordvectors) \n",
    "\n",
    "우리의 목표는 데이터를 ans_corpus의 aug를 통해 2배로 늘리고 que_corpus의 aug를 통해 2배로 늘려서 \n",
    "\n",
    "총 양을 3배 정도로 늘리는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import os\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "kobin_path = os.getenv('HOME')+'/aiffel/transformer_chatbot/ko.bin'\n",
    "word2vec = gensim.models.Word2Vec.load(kobin_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_sub(sentence, word2vec):\n",
    "    import random\n",
    "\n",
    "    res = []\n",
    "    toks = sentence\n",
    "\n",
    "    try:\n",
    "        _from = random.choice(toks)\n",
    "        _to = word2vec.most_similar(_from)[0][0]\n",
    "\n",
    "    except:   # 단어장에 없는 단어\n",
    "        return None\n",
    "\n",
    "    for tok in toks:\n",
    "        if tok is _from: res.append(_to)\n",
    "        else: res.append(tok)\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7648 7648\n"
     ]
    }
   ],
   "source": [
    "print(len(que_corpus), len(ans_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssac4/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "que_corpus_aug = que_corpus\n",
    "ans_corpus_aug = ans_corpus\n",
    "for idx in range(len(que_corpus)):\n",
    "    q = lexical_sub(que_corpus[idx],word2vec)\n",
    "    a = lexical_sub(ans_corpus[idx],word2vec)\n",
    "    if a != None:\n",
    "        que_corpus_aug.append(que_corpus[idx])\n",
    "        ans_corpus_aug.append(a)\n",
    "    if q != None:\n",
    "        que_corpus_aug.append(q)\n",
    "        ans_corpus_aug.append(ans_corpus[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래와 같이 augmentation된게 추가되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['12', '시', '땡', '!']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "que_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['12', '시', '끗', '!']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "que_corpus_aug[7649]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['여행', '은', '언제나', '좋', '죠', '.']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_corpus[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['여행', '은', '항상', '좋', '죠', '.']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_corpus_aug[7650]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20953 20953\n"
     ]
    }
   ],
   "source": [
    "print(len(ans_corpus_aug),len(que_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<start>', '하루', '가', '또', '가', '네요', '.', '<end>'], ['<start>', '위로', '해', '드립니다', '.', '<end>'], ['<start>', '여행', '은', '언제나', '좋', '죠', '.', '<end>'], ['<start>', '눈살', '이', '찌푸려', '지', '죠', '.', '<end>'], ['<start>', '다시', '새로', '사', '는', '게', '마음', '편해요', '.', '<end>']]\n",
      "\n",
      "\n",
      "[['<start>', '12', '시', '땡', '!', '<end>'], ['<start>', '1', '지망', '학교', '떨어졌', '어', '<end>'], ['<start>', '3', '박', '4', '일', '놀', '러', '가', '고', '싶', '다', '<end>'], ['<start>', 'ppl', '심하', '네', '<end>'], ['<start>', 'sd', '카드', '망가졌', '어', '<end>']]\n"
     ]
    }
   ],
   "source": [
    "ans_corpus_vectored = []\n",
    "que_corpus_vectored = []\n",
    "for i in range(len(ans_corpus_aug)):\n",
    "    ans_corpus_vectored.append(['<start>'] + ans_corpus_aug[i] + ['<end>'])\n",
    "    \n",
    "for i in range(len(que_corpus_aug)):\n",
    "    que_corpus_vectored.append(['<start>'] + que_corpus_aug[i] + ['<end>'])\n",
    "    \n",
    "print(ans_corpus_vectored[:5])\n",
    "print('\\n')\n",
    "print(que_corpus_vectored[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = ans_corpus_vectored + que_corpus_vectored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "words = np.concatenate(all_data).tolist()\n",
    "counter = Counter(words)\n",
    "counter = counter.most_common()\n",
    "vocab = ['<pad>', '<unk>']  + [key for key, _ in counter]\n",
    "word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7123"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word_to_index)+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_sentence(sentence, word_to_index):\n",
    "    return [word_to_index[word] if word in word_to_index else word_to_index['<unk>'] for word in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(corpus, word_to_index):\n",
    "    vectored = []\n",
    "    for sen in corpus:\n",
    "        sen = encoding_sentence(sen, word_to_index)\n",
    "        vectored.append(sen)\n",
    "    return vectored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "sou = vectorize(que_corpus_vectored, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt = vectorize(ans_corpus_vectored, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 2512, 214, 3717, 111, 3],\n",
       " [2, 309, 6778, 647, 1204, 13, 3],\n",
       " [2, 299, 3216, 527, 67, 289, 286, 9, 11, 38, 40, 3],\n",
       " [2, 4572, 1250, 43, 3],\n",
       " [2, 4573, 1699, 4574, 13, 3],\n",
       " [2, 602, 197, 1224, 137, 45, 7, 20, 3],\n",
       " [2, 602, 74, 1692, 98, 14, 21, 25, 411, 7, 6, 155, 3],\n",
       " [2, 602, 18, 41, 23, 66, 1215, 11, 40, 166, 17, 576, 3],\n",
       " [2, 507, 332, 17, 3],\n",
       " [2, 507, 16, 187, 98, 24, 10, 40, 3],\n",
       " [2, 5966, 34, 223, 44, 5967, 3],\n",
       " [2, 5968, 15, 251, 1634, 2006, 3],\n",
       " [2, 4575, 4576, 4577, 2998, 3],\n",
       " [2, 2513, 1107, 1738, 11, 1878, 13, 3],\n",
       " [2, 2513, 363, 55, 76, 856, 40, 4, 3],\n",
       " [2, 2513, 363, 5969, 1106, 1321, 26, 13, 3],\n",
       " [2, 639, 296, 9, 64, 96, 60, 13, 3],\n",
       " [2, 639, 1987, 296, 840, 4, 3],\n",
       " [2, 639, 69, 18, 11, 38, 13, 3],\n",
       " [2, 639, 69, 158, 5970, 17, 3],\n",
       " [2, 639, 69, 158, 296, 518, 14, 88, 3],\n",
       " [2, 639, 78, 550, 680, 62, 13, 3],\n",
       " [2, 3402, 122, 19, 3],\n",
       " [2, 3402, 115, 518, 139, 9, 37, 13, 3],\n",
       " [2, 2123, 1091, 311, 10, 1037, 3],\n",
       " [2, 2123, 1019, 155, 3],\n",
       " [2, 2123, 1635, 155, 3],\n",
       " [2, 2413, 105, 103, 93, 3],\n",
       " [2, 4578, 4579, 2972, 20, 3],\n",
       " [2, 4580, 171, 13, 3],\n",
       " [2, 865, 51, 499, 186, 168, 3],\n",
       " [2, 1106, 3718, 3],\n",
       " [2, 1106, 1971, 36, 33, 21, 3],\n",
       " [2, 1106, 9, 163, 892, 3],\n",
       " [2, 1424, 3322, 1288, 10, 21, 3],\n",
       " [2, 201, 5, 4581, 344, 4582, 5, 88, 3],\n",
       " [2, 201, 2212, 8, 110, 7, 26, 13, 3],\n",
       " [2, 1792, 23, 31, 559, 7, 6, 97, 9, 15, 13, 3],\n",
       " [2, 420, 421, 47, 5, 1105, 69, 1037, 3],\n",
       " [2, 420, 529, 23, 3],\n",
       " [2, 420, 5971, 885, 60, 13, 3],\n",
       " [2, 420, 1174, 34, 327, 9, 307, 14, 33, 21, 3],\n",
       " [2, 1452, 34, 3403, 5972, 7, 25, 3],\n",
       " [2, 1700, 904, 11, 38, 13, 3],\n",
       " [2, 1700, 904, 11, 38, 113, 1222, 45, 168, 820, 3],\n",
       " [2, 5973, 9, 149, 1480, 19, 3],\n",
       " [2, 159, 571, 7, 243, 115, 54, 23, 19, 3],\n",
       " [2, 159, 289, 286, 518, 46, 9, 37, 13, 3],\n",
       " [2, 159, 103, 62, 25, 23, 66, 2999, 14, 33, 21, 3],\n",
       " [2, 159, 4583, 9, 64, 96, 60, 13, 3],\n",
       " [2, 159, 15, 80, 205, 139, 452, 11, 38, 13, 3],\n",
       " [2, 159, 971, 4584, 9, 243, 17, 277, 19, 3],\n",
       " [2, 159, 53, 39, 15, 6, 731, 594, 105, 15, 93, 3],\n",
       " [2, 1701, 2275, 348, 13, 277, 3],\n",
       " [2, 1701, 5, 40, 3],\n",
       " [2, 1701, 5, 1157, 3],\n",
       " [2, 1040, 33, 16, 376, 3],\n",
       " [2, 5974, 5975, 13, 3],\n",
       " [2, 2192, 22, 422, 11, 2007, 3],\n",
       " [2, 1040, 885, 3],\n",
       " [2, 1183, 104, 98, 1580, 210, 40, 768, 3],\n",
       " [2, 1040, 1428, 3],\n",
       " [2, 1040, 10, 21, 3],\n",
       " [2, 4585, 7, 311, 10, 40, 3],\n",
       " [2, 965, 55, 171, 40, 3],\n",
       " [2, 965, 6, 2514, 1323, 199, 189, 14, 153, 19, 3],\n",
       " [2, 965, 138, 33, 16, 383, 252, 80, 10, 26, 40, 3],\n",
       " [2, 2446, 159, 67, 17, 422, 11, 521, 13, 3],\n",
       " [2, 2446, 252, 13, 3],\n",
       " [2, 611, 60, 13, 3],\n",
       " [2, 611, 8, 23, 22, 114, 24, 222, 17, 3],\n",
       " [2, 611, 5, 611, 8, 1901, 21, 3],\n",
       " [2, 267, 460, 134, 1547, 3],\n",
       " [2, 480, 641, 3],\n",
       " [2, 480, 437, 1671, 728, 3],\n",
       " [2, 480, 4586, 167, 13, 3],\n",
       " [2, 480, 5, 735, 3],\n",
       " [2, 480, 7, 24, 1178, 7, 6, 146, 3],\n",
       " [2, 3000, 3000, 21, 6, 28, 152, 65, 115, 216, 19, 3],\n",
       " [2, 5976, 56, 54, 11, 38, 13, 3],\n",
       " [2, 6779, 3],\n",
       " [2, 5977, 4587, 3],\n",
       " [2, 3404, 7, 43, 3],\n",
       " [2, 5978, 22, 131, 168, 3],\n",
       " [2, 119, 13, 9, 11, 15, 25, 3405, 225, 966, 3],\n",
       " [2, 5979, 3],\n",
       " [2, 4588, 2417, 9, 15, 13, 3],\n",
       " [2, 571, 161, 27, 74, 40, 521, 13, 3],\n",
       " [2, 571, 161, 27, 855, 5, 585, 1775, 20, 6, 237, 3],\n",
       " [2, 571, 707, 15, 13, 4, 3],\n",
       " [2, 571, 4589, 3],\n",
       " [2, 1439, 648, 604, 5, 88, 3],\n",
       " [2, 1439, 27, 6, 4590, 5, 20, 111, 3],\n",
       " [2, 2462, 87, 17, 310, 3],\n",
       " [2, 2798, 2396, 539, 103, 923, 4, 3],\n",
       " [2, 343, 396, 517, 5, 1157, 1000, 40, 3],\n",
       " [2, 314, 110, 7, 26, 13, 4, 3],\n",
       " [2, 314, 16, 1280, 41, 3363, 829, 10, 26, 20, 19, 3],\n",
       " [2, 314, 8, 110, 968, 26, 13, 4, 779, 17, 3],\n",
       " [2, 314, 104, 98, 4591, 5, 37, 13, 3],\n",
       " [2, 142, 60, 25, 4, 3],\n",
       " [2, 142, 22, 40, 284, 5, 40, 4, 3],\n",
       " [2, 1205, 9, 64, 478, 21, 3],\n",
       " [2, 1205, 84, 3406, 5, 37, 8, 499, 179, 267, 168, 3],\n",
       " [2, 1205, 5, 55, 95, 21, 3],\n",
       " [2, 142, 432, 7, 20, 137, 222, 23, 78, 503, 61, 3001, 111, 3],\n",
       " [2, 142, 185, 7, 6, 139, 284, 945, 397, 23, 3],\n",
       " [2, 142, 7, 6, 139, 284, 76, 397, 43, 3],\n",
       " [2, 142, 7, 41, 10, 21, 19, 3],\n",
       " [2, 142, 7, 41, 10, 93, 3],\n",
       " [2, 142, 7, 41, 166, 17, 19, 3],\n",
       " [2, 142, 122, 3],\n",
       " [2, 142, 115, 216, 3],\n",
       " [2, 2035, 5, 55, 2349, 17, 3],\n",
       " [2, 211, 337, 115, 216, 3],\n",
       " [2, 211, 753, 7, 6, 14, 478, 21, 3],\n",
       " [2, 211, 948, 5, 41, 10, 8, 345, 3],\n",
       " [2, 211, 18, 11, 38, 13, 3],\n",
       " [2, 211, 316, 5, 240, 45, 10, 21, 3],\n",
       " [2, 211, 5980, 6, 386, 3],\n",
       " [2, 211, 1501, 7, 11, 38, 13, 3],\n",
       " [2, 211, 3002, 66, 558, 3],\n",
       " [2, 11, 299, 16, 337, 66, 2515, 4, 3],\n",
       " [2, 5981, 1178, 728, 3],\n",
       " [2, 2565, 2516, 103, 11, 38, 40, 4, 3],\n",
       " [2, 4592, 64, 1548, 13, 3],\n",
       " [2, 1844, 34, 553, 3],\n",
       " [2, 2361, 28, 69, 5, 95, 21, 3],\n",
       " [2, 4593, 3578, 1050, 80, 884, 20, 3],\n",
       " [2, 186, 15, 13, 3],\n",
       " [2, 160, 7, 11, 175, 7, 41, 312, 7, 20, 3],\n",
       " [2, 3407, 55, 375, 17, 3],\n",
       " [2, 3407, 85, 354, 11, 38, 13, 3],\n",
       " [2, 3003, 2517, 18, 6, 155, 3],\n",
       " [2, 3003, 904, 11, 38, 13, 3],\n",
       " [2, 5982, 6, 627, 519, 31, 34, 14, 33, 21, 3],\n",
       " [2, 3719, 3004, 28, 3],\n",
       " [2, 3408, 110, 1111, 25, 3],\n",
       " [2, 3408, 4594, 168, 3],\n",
       " [2, 4595, 103, 11, 38, 13, 4, 3],\n",
       " [2, 1615, 82, 26, 20, 3],\n",
       " [2, 1615, 54, 11, 38, 40, 3],\n",
       " [2, 1615, 947, 337, 70, 40, 3],\n",
       " [2, 1615, 947, 355, 8, 14, 33, 21, 3],\n",
       " [2, 1615, 185, 122, 3],\n",
       " [2, 3409, 5, 143, 248, 5, 750, 1229, 3],\n",
       " [2, 3409, 5, 41, 750, 282, 19, 3],\n",
       " [2, 337, 211, 115, 216, 3],\n",
       " [2, 337, 372, 131, 122, 3],\n",
       " [2, 337, 194, 115, 216, 3],\n",
       " [2, 337, 137, 131, 168, 19, 3],\n",
       " [2, 337, 57, 45, 168, 3],\n",
       " [2, 337, 57, 7, 11, 38, 13, 3],\n",
       " [2, 337, 87, 63, 53, 119, 3],\n",
       " [2, 337, 146, 5, 401, 307, 235, 19, 3],\n",
       " [2, 337, 7, 64, 171, 13, 3],\n",
       " [2, 1776, 185, 70, 13, 3],\n",
       " [2, 1776, 185, 155, 3],\n",
       " [2, 4596, 18, 11, 38, 13, 3],\n",
       " [2, 4597, 3005, 23, 66, 70, 13, 19, 3],\n",
       " [2, 3006, 3007, 351, 13, 4, 3],\n",
       " [2, 3008, 27, 6, 232, 5, 735, 3],\n",
       " [2, 1117, 6, 121, 11, 303, 77, 23, 21, 9, 866, 3],\n",
       " [2, 1721, 225, 2389, 9, 45, 168, 3],\n",
       " [2, 1721, 60, 23, 179, 3],\n",
       " [2, 1721, 60, 40, 3],\n",
       " [2, 144, 4598, 315, 54, 26, 20, 19, 3],\n",
       " [2, 1490, 103, 11, 223, 866, 3],\n",
       " [2, 1490, 45, 103, 24, 168, 3],\n",
       " [2, 392, 9, 211, 1193, 7, 40, 4, 3],\n",
       " [2, 283, 1039, 127, 7, 11, 38, 40, 4, 3],\n",
       " [2, 5983, 3718, 3],\n",
       " [2, 4599, 9, 45, 1220, 3],\n",
       " [2, 82, 21, 20, 11, 15, 13, 3],\n",
       " [2, 82, 16, 28, 152, 180, 271, 171, 13, 3],\n",
       " [2, 880, 2224, 20, 51, 127, 3],\n",
       " [2, 880, 1493, 13, 3],\n",
       " [2, 880, 2674, 225, 1548, 40, 3],\n",
       " [2, 880, 5984, 74, 743, 40, 3],\n",
       " [2, 880, 4600, 17, 3],\n",
       " [2, 4601, 5, 54, 13, 9, 6, 386, 5, 69, 13, 3],\n",
       " [2, 4602, 167, 13, 3],\n",
       " [2, 1549, 1702, 701, 13, 3],\n",
       " [2, 1549, 1702, 74, 27, 52, 27, 397, 6, 97, 15, 13, 3],\n",
       " [2, 1549, 1702, 1505, 707, 1139, 3],\n",
       " [2, 5985, 207, 122, 3],\n",
       " [2, 5986, 4603, 13, 4, 3],\n",
       " [2, 1550, 9, 64, 171, 13, 3],\n",
       " [2, 1550, 521, 40, 388, 13, 3],\n",
       " [2, 3198, 7, 174, 560, 67, 768, 3],\n",
       " [2, 974, 521, 40, 350, 84, 210, 1041, 39, 15, 93, 3],\n",
       " [2, 974, 574, 56, 329, 3],\n",
       " [2, 974, 276, 123, 4604, 3],\n",
       " [2, 5987, 3],\n",
       " [2, 332, 7, 41, 163, 4605, 612, 3],\n",
       " [2, 332, 17, 3],\n",
       " [2, 669, 610, 3],\n",
       " [2, 669, 9, 4606, 3],\n",
       " [2, 4607, 549, 19, 3],\n",
       " [2, 92, 28, 5, 23, 45, 173, 6, 14, 33, 21, 3],\n",
       " [2, 92, 28, 5, 23, 10, 21, 17, 479, 80, 10, 26, 40, 3],\n",
       " [2, 92, 28, 5, 166, 1740, 10, 26, 40, 3],\n",
       " [2, 92, 5988, 775, 40, 1055, 20, 3],\n",
       " [2, 278, 4608, 4609, 56, 62, 80, 3],\n",
       " [2, 278, 530, 277, 19, 3],\n",
       " [2, 278, 263, 11, 38, 40, 3],\n",
       " [2, 278, 727, 458, 1616, 4, 3],\n",
       " [2, 278, 320, 134, 11, 38, 13, 3],\n",
       " [2, 278, 223, 6, 14, 116, 20, 19, 3],\n",
       " [2, 278, 2008, 120, 866, 4, 3],\n",
       " [2, 278, 122, 19, 3],\n",
       " [2, 278, 187, 486, 432, 103, 923, 3],\n",
       " [2, 278, 187, 15, 6, 24, 10, 21, 3],\n",
       " [2, 92, 619, 57, 1419, 215, 19, 3],\n",
       " [2, 879, 107, 223, 3],\n",
       " [2, 879, 108, 314, 60, 13, 3],\n",
       " [2, 505, 87, 393, 60, 25, 3],\n",
       " [2, 189, 51, 8, 137, 7, 20, 3],\n",
       " [2, 189, 28, 177, 181, 131, 206, 179, 3],\n",
       " [2, 189, 46, 116, 62, 25, 55, 478, 24, 7, 43, 3],\n",
       " [2, 308, 518, 14, 174, 3],\n",
       " [2, 308, 313, 134, 81, 25, 22, 275, 31, 110, 7, 26, 13, 3],\n",
       " [2, 3009, 57, 2009, 11, 38, 40, 3],\n",
       " [2, 423, 230, 11, 593, 11, 38, 13, 3],\n",
       " [2, 423, 103, 458, 7, 25, 3],\n",
       " [2, 423, 134, 11, 38, 13, 3],\n",
       " [2, 1374, 189, 302, 3],\n",
       " [2, 4610, 34, 293, 90, 21, 1973, 13, 3],\n",
       " [2, 2118, 15, 80, 1212, 8, 345, 3],\n",
       " [2, 5989, 90, 21, 19, 3],\n",
       " [2, 2568, 4611, 177, 3],\n",
       " [2, 4612, 100, 196, 11, 1803, 41, 10, 26, 20, 19, 3],\n",
       " [2, 2568, 3814, 96, 5990, 3],\n",
       " [2, 2163, 5, 233, 20, 49, 21, 3],\n",
       " [2, 64, 5991, 13, 3],\n",
       " [2, 975, 40, 2182, 6, 14, 478, 21, 3],\n",
       " [2, 975, 140, 673, 62, 13, 3],\n",
       " [2, 4559, 87, 1160, 179, 179, 3],\n",
       " [2, 301, 6, 36, 22, 1182, 3],\n",
       " [2, 301, 127, 51, 257, 26, 13, 3],\n",
       " [2, 2124, 5, 485, 5, 252, 23, 179, 3],\n",
       " [2, 393, 9, 1617, 13, 3],\n",
       " [2, 393, 9, 315, 605, 139, 4613, 39, 15, 6, 146, 15, 93, 19, 3],\n",
       " [2, 393, 7, 11, 15, 62, 25, 3],\n",
       " [2, 2254, 39, 15, 6, 28, 3],\n",
       " [2, 2229, 5, 37, 13, 3],\n",
       " [2, 3010, 2367, 3011, 13, 4, 3],\n",
       " [2, 784, 5, 690, 22, 114, 6, 36, 33, 21, 4, 3],\n",
       " [2, 4614, 87, 60, 32, 3],\n",
       " [2, 226, 4615, 17, 3],\n",
       " [2, 226, 421, 2674, 8, 211, 7, 11, 15, 13, 3],\n",
       " [2, 226, 1309, 225, 87, 1016, 11, 15, 13, 3],\n",
       " [2, 226, 853, 7, 11, 38, 13, 3],\n",
       " [2, 226, 5, 3410, 33, 21, 3],\n",
       " [2, 226, 5, 3411, 3],\n",
       " [2, 226, 5, 5992, 17, 3],\n",
       " [2, 226, 5, 268, 17, 3],\n",
       " [2, 2675, 82, 93, 3],\n",
       " [2, 2675, 1204, 13, 3],\n",
       " [2, 1492, 2676, 3],\n",
       " [2, 4616, 120, 11, 296, 9, 11, 38, 13, 3],\n",
       " [2, 3412, 22, 257, 26, 13, 3],\n",
       " [2, 2294, 2518, 99, 81, 13, 111, 3],\n",
       " [2, 2294, 2518, 198, 82, 93, 19, 3],\n",
       " [2, 2294, 2518, 77, 198, 99, 81, 13, 3],\n",
       " [2, 781, 31, 3413, 13, 3],\n",
       " [2, 4617, 311, 1981, 800, 97, 69, 95, 26, 20, 3],\n",
       " [2, 271, 336, 641, 412, 40, 4, 3],\n",
       " [2, 1344, 1502, 6, 552, 1160, 3],\n",
       " [2, 1344, 1509, 4618, 43, 3],\n",
       " [2, 2489, 85, 1187, 196, 13, 18, 41, 1045, 54, 23, 3],\n",
       " [2, 79, 85, 759, 1481, 288, 13, 3],\n",
       " [2, 79, 16, 786, 11, 17, 6, 2295, 3],\n",
       " [2, 79, 5, 4619, 4620, 1618, 60, 13, 3],\n",
       " [2, 79, 5, 45, 576, 3],\n",
       " [2, 79, 5, 1297, 62, 13, 3],\n",
       " [2, 1421, 2519, 1062, 103, 11, 38, 13, 4, 3],\n",
       " [2, 4621, 22, 37, 43, 3],\n",
       " [2, 4622, 103, 923, 3],\n",
       " [2, 5993, 103, 11, 38, 13, 3],\n",
       " [2, 499, 21, 777, 69, 5994, 997, 3],\n",
       " [2, 3405, 683, 2732, 45, 3012, 43, 3],\n",
       " [2, 4623, 3013, 14, 33, 21, 3],\n",
       " [2, 4624, 4625, 382, 36, 33, 113, 3],\n",
       " [2, 977, 99, 11, 38, 40, 3],\n",
       " [2, 977, 191, 11, 38, 13, 3],\n",
       " [2, 977, 198, 10, 21, 122, 3],\n",
       " [2, 977, 597, 24, 4626, 13, 3],\n",
       " [2, 977, 24, 4627, 489, 40, 4, 3],\n",
       " [2, 5995, 870, 6, 155, 3],\n",
       " [2, 5996, 9, 11, 38, 13, 3],\n",
       " [2, 2114, 4628, 3014, 3],\n",
       " [2, 2114, 99, 81, 13, 3],\n",
       " [2, 2114, 1337, 13, 3],\n",
       " [2, 2114, 198, 82, 20, 19, 3],\n",
       " [2, 4629, 198, 158, 1490, 3689, 198, 105, 9, 10, 21, 19, 3],\n",
       " [2, 1373, 2065, 3],\n",
       " [2, 192, 16, 95, 113, 3],\n",
       " [2, 192, 5, 55, 4630, 13, 3],\n",
       " [2, 192, 5, 1072, 17, 3],\n",
       " [2, 192, 5, 230, 1040, 88, 3],\n",
       " [2, 192, 5, 37, 13, 3],\n",
       " [2, 192, 5, 899, 13, 1126, 19, 3],\n",
       " [2, 123, 23, 311, 1585, 7, 40, 3],\n",
       " [2, 123, 23, 41, 10, 8, 203, 90, 81, 25, 4, 3],\n",
       " [2, 4631, 9, 268, 7, 678, 522, 2677, 13, 3],\n",
       " [2, 23, 470, 11, 5997, 377, 3],\n",
       " [2, 23, 33, 16, 28, 16, 1109, 904, 41, 45, 54, 26, 20, 3],\n",
       " [2, 23, 159, 800, 97, 31, 137, 4632, 3],\n",
       " [2, 23, 611, 110, 7, 26, 13, 3],\n",
       " [2, 23, 314, 57, 34, 14, 20, 19, 3],\n",
       " [2, 23, 82, 20, 49, 153, 3],\n",
       " [2, 23, 1147, 4633, 14, 153, 3],\n",
       " [2, 23, 335, 2675, 9, 88, 168, 3],\n",
       " [2, 23, 61, 4634, 66, 1381, 3],\n",
       " [2, 23, 55, 110, 351, 13, 3],\n",
       " [2, 23, 55, 1879, 17, 3],\n",
       " [2, 23, 1906, 191, 310, 3],\n",
       " [2, 23, 5998, 64, 3637, 19, 3],\n",
       " [2, 23, 285, 24, 19, 3],\n",
       " [2, 23, 3414, 252, 23, 179, 3],\n",
       " [2, 23, 132, 14, 122, 3],\n",
       " [2, 23, 3415, 34, 14, 627, 3],\n",
       " [2, 23, 4635, 503, 7, 6, 97, 133, 122, 19, 3],\n",
       " [2, 23, 51, 519, 34, 14, 33, 21, 4, 3],\n",
       " [2, 23, 626, 316, 6, 14, 33, 21, 3],\n",
       " [2, 23, 336, 421, 237, 3],\n",
       " [2, 23, 190, 57, 24, 3],\n",
       " [2, 23, 114, 6, 24, 137, 320, 95, 20, 3],\n",
       " [2, 23, 1269, 180, 6, 14, 33, 97, 3],\n",
       " [2, 23, 559, 1690, 14, 33, 21, 3],\n",
       " [2, 23, 559, 7, 6, 28, 133, 17, 19, 3],\n",
       " [2, 23, 410, 9, 95, 16, 14, 33, 21, 3],\n",
       " [2, 23, 105, 7, 6, 14, 20, 3],\n",
       " [2, 23, 2296, 997, 111, 3],\n",
       " [2, 23, 1005, 11, 38, 13, 3],\n",
       " [2, 23, 660, 740, 177, 604, 3],\n",
       " [2, 23, 3015, 88, 3],\n",
       " [2, 23, 3720, 99, 16, 14, 33, 21, 3],\n",
       " [2, 23, 2678, 2679, 1690, 14, 33, 113, 312, 17, 19, 3],\n",
       " [2, 23, 4636, 5, 88, 3],\n",
       " [2, 23, 30, 7, 153, 19, 3],\n",
       " [2, 23, 891, 99, 3016, 111, 3],\n",
       " [2, 23, 531, 644, 1337, 40, 3],\n",
       " [2, 23, 316, 16, 14, 33, 21, 3],\n",
       " [2, 23, 316, 16, 237, 3],\n",
       " [2, 23, 2680, 840, 3],\n",
       " [2, 23, 4637, 1513, 177, 179, 3],\n",
       " [2, 23, 3416, 60, 13, 3],\n",
       " [2, 23, 519, 34, 377, 3],\n",
       " [2, 23, 5999, 177, 3],\n",
       " [2, 23, 170, 2083, 445, 14, 33, 21, 3],\n",
       " [2, 23, 543, 53, 14, 88, 3],\n",
       " [2, 23, 109, 6000, 962, 3],\n",
       " [2, 23, 109, 330, 57, 1703, 378, 43, 3],\n",
       " [2, 23, 109, 891, 99, 81, 6001, 3],\n",
       " [2, 23, 796, 2178, 1235, 13, 111, 3],\n",
       " [2, 23, 2520, 88, 3],\n",
       " [2, 23, 2520, 98, 14, 33, 21, 3],\n",
       " [2, 23, 137, 2080, 17, 3],\n",
       " [2, 23, 502, 568, 422, 11, 134, 11, 15, 6, 14, 33, 21, 3],\n",
       " [2, 23, 649, 103, 6, 14, 33, 21, 3],\n",
       " [2, 23, 3417, 179, 3],\n",
       " [2, 23, 268, 405, 3],\n",
       " [2, 23, 268, 17, 19, 3],\n",
       " [2, 23, 108, 1177, 17, 3],\n",
       " [2, 23, 1566, 11, 38, 13, 3],\n",
       " [2, 23, 57, 134, 39, 15, 26, 20, 3],\n",
       " [2, 23, 57, 351, 20, 19, 3],\n",
       " [2, 23, 57, 7, 11, 15, 6, 447, 114, 26, 13, 3],\n",
       " [2, 23, 57, 7, 6, 24, 37, 13, 3],\n",
       " [2, 23, 57, 7, 6, 24, 37, 6, 14, 33, 21, 3],\n",
       " [2, 23, 57, 53, 39, 15, 93, 3],\n",
       " [2, 23, 568, 1211, 24, 51, 17, 310, 3],\n",
       " [2, 23, 87, 2224, 20, 964, 3],\n",
       " [2, 23, 87, 10, 21, 17, 479, 80, 3],\n",
       " [2, 23, 87, 4638, 6, 237, 3],\n",
       " [2, 23, 87, 769, 17, 310, 3],\n",
       " [2, 23, 10, 21, 7, 24, 273, 11, 38, 40, 3],\n",
       " [2, 23, 10, 21, 17, 56, 6, 28, 15, 26, 20, 19, 3],\n",
       " [2, 23, 3418, 15, 23, 19, 3],\n",
       " [2, 23, 355, 8, 1618, 282, 3],\n",
       " [2, 23, 4639, 14, 33, 21, 3],\n",
       " [2, 23, 2037, 33, 21, 3],\n",
       " [2, 23, 1909, 6, 240, 57, 17, 3],\n",
       " [2, 23, 46, 69, 78, 1566, 11, 38, 13, 3],\n",
       " [2, 23, 4640, 98, 237, 3],\n",
       " [2, 23, 855, 1513, 98, 14, 33, 97, 3],\n",
       " [2, 23, 855, 4641, 55, 76, 17, 3],\n",
       " [2, 23, 53, 39, 15, 13, 3],\n",
       " [2, 23, 1978, 51, 7, 6, 14, 887, 11, 38, 13, 3],\n",
       " [2, 23, 187, 1307, 17, 3],\n",
       " [2, 23, 187, 296, 167, 25, 82, 43, 3],\n",
       " [2, 23, 187, 149, 1909, 3017, 3],\n",
       " [2, 23, 1021, 8, 55, 110, 17, 3],\n",
       " [2, 23, 533, 85, 1566, 11, 38, 13, 3],\n",
       " [2, 354, 64, 22, 478, 21, 3],\n",
       " [2, 23, 6, 4642, 191, 6, 14, 33, 21, 3],\n",
       " [2, 23, 6, 6002, 28, 98, 14, 33, 21, 3],\n",
       " [2, 23, 6, 575, 57, 122, 3],\n",
       " [2, 23, 6, 137, 320, 3188, 93, 19, 3],\n",
       " [2, 23, 6, 137, 4643, 3],\n",
       " [2, 23, 6, 57, 53, 203, 21, 6, 24, 37, 6, 36, 33, 21, 3],\n",
       " [2, 23, 6, 10, 21, 7, 6, 24, 566, 3],\n",
       " [2, 23, 6, 10, 113, 4, 3],\n",
       " [2, 23, 6, 46, 9, 37, 13, 3],\n",
       " [2, 23, 6, 46, 127, 236, 62, 25, 3],\n",
       " [2, 23, 22, 82, 16, 28, 152, 3],\n",
       " [2, 23, 22, 2366, 99, 11, 38, 193, 3],\n",
       " [2, 23, 22, 4644, 153, 348, 11, 38, 40, 3],\n",
       " [2, 23, 22, 891, 99, 11, 38, 40, 3],\n",
       " [2, 23, 22, 4645, 4646, 277, 19, 3],\n",
       " [2, 23, 22, 1077, 124, 17, 3],\n",
       " [2, 23, 22, 888, 96, 99, 11, 38, 40, 3],\n",
       " [2, 23, 22, 1998, 9, 54, 513, 111, 3],\n",
       " [2, 23, 22, 3018, 4647, 17, 277, 19, 3],\n",
       " [2, 23, 22, 232, 191, 11, 38, 13, 3],\n",
       " [2, 23, 22, 889, 2275, 348, 11, 38, 40, 3],\n",
       " [2, 4648, 31, 9, 277, 3],\n",
       " [2, 23, 538, 857, 3],\n",
       " [2, 23, 138, 289, 21, 310, 3],\n",
       " [2, 23, 138, 289, 223, 3],\n",
       " [2, 23, 138, 635, 37, 6, 228, 69, 3],\n",
       " [2, 23, 138, 15, 6, 24, 70, 62, 23, 179, 3],\n",
       " [2, 4649, 7, 40, 3],\n",
       " [2, 23, 31, 574, 479, 80, 10, 26, 40, 3],\n",
       " [2, 23, 31, 55, 313, 301, 24, 60, 13, 3],\n",
       " [2, 23, 31, 55, 1978, 663, 3],\n",
       " [2, 23, 31, 2740, 831, 24, 1214, 239, 3],\n",
       " [2, 23, 31, 2589, 39, 15, 6, 83, 105, 9, 15, 93, 3],\n",
       " [2, 23, 31, 46, 96, 47, 45, 60, 23, 179, 3],\n",
       " [2, 23, 31, 3419, 96, 21, 6, 28, 312, 17, 19, 3],\n",
       " [2, 23, 66, 192, 460, 191, 6, 14, 33, 21, 3],\n",
       " [2, 23, 66, 262, 37, 13, 3],\n",
       " [2, 23, 66, 4650, 6, 386, 5, 88, 3],\n",
       " [2, 23, 66, 383, 46, 37, 13, 3],\n",
       " [2, 23, 66, 1215, 11, 166, 17, 576, 3],\n",
       " [2, 23, 66, 583, 23, 3],\n",
       " [2, 23, 66, 1619, 179, 3],\n",
       " [2, 23, 66, 268, 34, 28, 1381, 3],\n",
       " [2, 23, 66, 1820, 1738, 149, 623, 3420, 3],\n",
       " [2, 23, 66, 1100, 1240, 5, 88, 3],\n",
       " [2, 23, 66, 1100, 98, 237, 3],\n",
       " [2, 23, 66, 1567, 110, 60, 13, 3],\n",
       " [2, 23, 66, 205, 14, 116, 20, 19, 3],\n",
       " [2, 23, 66, 44, 74, 5, 124, 34, 36, 33, 21, 3],\n",
       " [2, 23, 1215, 11, 40, 166, 34, 14, 33, 21, 3],\n",
       " [2, 421, 192, 1114, 13, 3],\n",
       " [2, 365, 161, 27, 559, 99, 81, 13, 3],\n",
       " [2, 365, 9, 95, 113, 1188, 5, 216, 3],\n",
       " [2, 365, 22, 15, 697, 3421, 87, 539, 277, 3],\n",
       " [2, 365, 69, 174, 529, 5, 95, 21, 509, 13, 3],\n",
       " [2, 365, 103, 1768, 3418, 1703, 3],\n",
       " [2, 814, 27, 105, 1880, 103, 11, 191, 551, 3],\n",
       " [2, 814, 27, 3721, 131, 820, 3],\n",
       " [2, 23, 78, 2660, 6, 24, 206, 22, 37, 62, 80, 3],\n",
       " [2, 23, 78, 1380, 23, 41, 884, 20, 19, 3],\n",
       " [2, 23, 78, 1380, 6003, 19, 3],\n",
       " [2, 23, 78, 55, 95, 16, 119, 340, 6, 237, 3],\n",
       " [2, 23, 78, 891, 44, 87, 7, 20, 3],\n",
       " [2, 23, 78, 53, 51, 15, 367, 566, 19, 3],\n",
       " [2, 23, 78, 1563, 87, 167, 80, 10, 26, 13, 3],\n",
       " [2, 23, 78, 66, 643, 1211, 967, 3],\n",
       " [2, 23, 78, 66, 137, 396, 67, 5, 4651, 3],\n",
       " [2, 6004, 4652, 6, 738, 10, 40, 3],\n",
       " [2, 2681, 45, 17, 430, 25, 3],\n",
       " [2, 2681, 701, 93, 3],\n",
       " [2, 2681, 10, 21, 7, 6, 59, 549, 19, 3],\n",
       " [2, 467, 76, 360, 34, 14, 33, 113, 3],\n",
       " [2, 467, 1087, 88, 3],\n",
       " [2, 467, 137, 597, 24, 51, 8, 110, 122, 3],\n",
       " [2, 467, 2037, 40, 3],\n",
       " [2, 2573, 5, 45, 168, 3],\n",
       " [2, 532, 3404, 34, 14, 33, 97, 3],\n",
       " [2, 532, 137, 320, 1742, 551, 3],\n",
       " [2, 532, 87, 6005, 14, 33, 21, 3],\n",
       " [2, 532, 10, 113, 3],\n",
       " [2, 532, 4653, 3],\n",
       " [2, 532, 3019, 10, 21, 3],\n",
       " [2, 532, 9, 55, 6006, 17, 3],\n",
       " [2, 532, 9, 55, 1667, 3],\n",
       " [2, 532, 9, 240, 3699, 40, 3],\n",
       " [2, 150, 21, 9, 11, 38, 13, 3],\n",
       " [2, 6007, 78, 222, 1852, 24, 54, 43, 3],\n",
       " [2, 71, 69, 94, 1566, 2297, 133, 131, 168, 19, 3],\n",
       " [2, 71, 16, 1289, 9, 37, 13, 3],\n",
       " [2, 71, 44, 259, 8, 55, 248, 737, 3],\n",
       " [2, 71, 44, 67, 534, 881, 122, 3],\n",
       " [2, 71, 44, 373, 2507, 62, 13, 61, 284, 3],\n",
       " [2, 71, 5, 1016, 20, 49, 6, 79, 8, 9, 329, 17, 3],\n",
       " [2, 59, 613, 287, 149, 89, 3],\n",
       " [2, 59, 46, 9, 3020, 479, 13, 3],\n",
       " [2, 59, 1021, 7, 6, 14, 549, 3],\n",
       " [2, 59, 41, 1026, 36, 33, 21, 3],\n",
       " [2, 59, 238, 68, 238, 1160, 3],\n",
       " [2, 59, 46, 1550, 1937, 11, 38, 13, 3],\n",
       " [2, 59, 46, 140, 358, 521, 13, 3],\n",
       " [2, 59, 46, 625, 152, 575, 687, 3],\n",
       " [2, 59, 46, 3416, 198, 96, 105, 9, 10, 93, 19, 3],\n",
       " [2, 59, 46, 109, 330, 6008, 17, 6009, 3],\n",
       " [2, 59, 46, 109, 87, 6010, 4, 3],\n",
       " [2, 59, 46, 9, 23, 45, 236, 13, 310, 3],\n",
       " [2, 59, 46, 9, 55, 1704, 3],\n",
       " [2, 59, 46, 9, 2298, 8, 55, 76, 4654, 3],\n",
       " [2, 59, 46, 9, 2682, 194, 1881, 3],\n",
       " [2, 59, 46, 9, 482, 2784, 3422, 3],\n",
       " [2, 59, 46, 9, 1682, 9, 95, 21, 3],\n",
       " [2, 59, 46, 9, 6011, 3],\n",
       " [2, 59, 46, 9, 1998, 17, 56, 41, 10, 26, 40, 4, 3],\n",
       " [2, 59, 46, 9, 348, 5, 4655, 3],\n",
       " [2, 59, 46, 9, 222, 2747, 17, 3],\n",
       " [2, 59, 46, 9, 374, 31, 57, 45, 17, 3],\n",
       " [2, 59, 46, 6, 287, 149, 89, 3],\n",
       " [2, 59, 46, 138, 2603, 1936, 17, 18, 329, 3],\n",
       " [2, 59, 46, 138, 732, 410, 96, 1841, 3],\n",
       " [2, 262, 602, 27, 61, 482, 37, 13, 3],\n",
       " [2, 262, 576, 687, 3],\n",
       " [2, 262, 2299, 27, 61, 482, 137, 45, 3423, 3],\n",
       " [2, 457, 5, 23, 45, 2300, 3],\n",
       " [2, 457, 5, 23, 181, 1471, 63, 57, 17, 3],\n",
       " [2, 457, 5, 626, 370, 24, 1482, 3],\n",
       " [2, 457, 5, 1918, 3],\n",
       " [2, 457, 5, 2301, 31, 45, 2848, 310, 4, 3],\n",
       " [2, 457, 5, 137, 97, 904, 6, 14, 45, 4656, 3],\n",
       " [2, 457, 5, 1471, 45, 534, 310, 4, 3],\n",
       " [2, 457, 5, 2683, 5, 127, 45, 169, 3],\n",
       " [2, 2363, 5, 2521, 36, 33, 21, 3],\n",
       " [2, 61, 59, 46, 2522, 5, 41, 10, 26, 40, 4, 3],\n",
       " [2, 61, 679, 5, 55, 1910, 3],\n",
       " [2, 61, 52, 8, 90, 21, 479, 80, 3],\n",
       " [2, 61, 472, 5, 653, 1040, 4657, 10, 26, 40, 3],\n",
       " [2, 61, 410, 6, 566, 3],\n",
       " [2, 61, 1720, 6, 287, 15, 93, 3],\n",
       " [2, 61, 3424, 55, 4658, 17, 3],\n",
       " [2, 61, 47, 158, 132, 28, 47, 5, 240, 404, 352, 119, 836, 3],\n",
       " [2, 61, 849, 116, 4659, 3],\n",
       " [2, 61, 493, 5, 3722, 23, 3],\n",
       " [2, 61, 1378, 255, 27, 45, 69, 13, 3],\n",
       " [2, 61, 1077, 66, 45, 2010, 3],\n",
       " [2, 61, 3723, 87, 633, 17, 479, 80, 3],\n",
       " [2, 61, 1015, 6, 1255, 23, 179, 3],\n",
       " [2, 61, 379, 476, 37, 13, 3],\n",
       " [2, 61, 379, 16, 4660, 33, 21, 3],\n",
       " [2, 61, 379, 44, 2644, 16, 6780, 3],\n",
       " [2, 61, 744, 418, 3],\n",
       " [2, 61, 401, 5, 690, 114, 26, 13, 3],\n",
       " [2, 61, 401, 98, 14, 33, 113, 51, 8, 110, 7, 26, 13, 3],\n",
       " [2, 61, 2576, 31, 114, 11, 6012, 377, 3],\n",
       " [2, 61, 4661, 78, 61, 4662, 60, 367, 3],\n",
       " [2, 61, 232, 5, 351, 13, 3],\n",
       " [2, 61, 75, 16, 287, 15, 93, 3],\n",
       " [2, 61, 9, 308, 692, 405, 3],\n",
       " [2, 61, 9, 6781, 14, 33, 21, 3],\n",
       " [2, 61, 9, 393, 31, 55, 76, 60, 23, 179, 3],\n",
       " [2, 61, 9, 55, 3201, 60, 13, 3],\n",
       " [2, 61, 9, 55, 4663, 51, 60, 13, 3],\n",
       " [2, 61, 9, 55, 233, 24, 2523, 23, 19, 3],\n",
       " [2, 61, 9, 55, 3021, 17, 3],\n",
       " [2, 61, 9, 132, 265, 51, 8, 7, 26, 13, 3],\n",
       " [2, 61, 9, 1573, 8, 110, 17, 3],\n",
       " [2, 61, 9, 76, 692, 405, 3],\n",
       " [2, 61, 9, 51, 7, 41, 137, 1666, 66, 122, 3],\n",
       " [2, 61, 9, 2080, 34, 14, 20, 3],\n",
       " [2, 61, 9, 886, 679, 7, 24, 2302, 3],\n",
       " [2, 61, 9, 1107, 4664, 88, 3],\n",
       " [2, 61, 9, 30, 53, 1131, 5, 15, 23, 3],\n",
       " [2, 61, 9, 4665, 6, 1342, 33, 21, 3],\n",
       " [2, 61, 9, 661, 22, 445, 28, 33, 21, 3],\n",
       " [2, 61, 9, 137, 131, 7, 183, 114, 26, 13, 3],\n",
       " [2, 61, 9, 612, 7, 6, 28, 5, 54, 64, 634, 3],\n",
       " [2, 61, 9, 268, 34, 377, 19, 3],\n",
       " [2, 61, 9, 401, 34, 235, 3],\n",
       " [2, 61, 9, 2639, 5, 116, 40, 3],\n",
       " [2, 61, 9, 10, 21, 7, 6, 14, 114, 23, 3],\n",
       " [2, 61, 9, 10, 21, 7, 6, 28, 144, 23, 31, 863, 56, 6, 28, 3],\n",
       " [2, 61, 9, 2576, 31, 833, 23, 179, 3],\n",
       " [2, 61, 9, 355, 251, 1915, 14, 33, 21, 3],\n",
       " [2, 61, 9, 240, 3267, 39, 15, 8, 24, 566, 3],\n",
       " [2, 61, 9, 6013, 367, 3],\n",
       " [2, 61, 9, 162, 2608, 14, 33, 21, 3],\n",
       " [2, 61, 9, 2749, 5, 252, 13, 3],\n",
       " [2, 61, 9, 205, 24, 95, 40, 3],\n",
       " [2, 6014, 225, 6015, 25, 1576, 105, 7, 20, 3],\n",
       " [2, 3609, 27, 6, 63, 166, 6016, 320, 1048, 182, 3],\n",
       " [2, 61, 52, 8, 114, 26, 13, 4, 3],\n",
       " [2, 61, 2395, 470, 11, 38, 13, 4, 3],\n",
       " [2, 335, 393, 7, 24, 54, 43, 3],\n",
       " [2, 335, 532, 549, 19, 3],\n",
       " [2, 335, 89, 243, 139, 263, 19, 3],\n",
       " [2, 335, 4666, 3017, 3],\n",
       " [2, 335, 1298, 23, 6, 139, 901, 3],\n",
       " [2, 335, 1298, 185, 3425, 3425, 3],\n",
       " [2, 335, 363, 167, 80, 3],\n",
       " [2, 335, 4667, 840, 3],\n",
       " [2, 335, 947, 5, 88, 3],\n",
       " [2, 335, 609, 15, 25, 532, 10, 81, 80, 3],\n",
       " [2, 335, 851, 654, 88, 168, 3],\n",
       " [2, 335, 46, 138, 289, 499, 19, 3],\n",
       " [2, 335, 2053, 4668, 2524, 3426, 43, 4, 3],\n",
       " [2, 335, 302, 1868, 1704, 3],\n",
       " [2, 3022, 41, 884, 20, 3],\n",
       " [2, 6017, 4669, 3],\n",
       " [2, 6018, 363, 55, 76, 558, 3],\n",
       " [2, 2377, 1519, 251, 103, 8, 24, 37, 43, 3],\n",
       " [2, 2377, 9, 4670, 62, 13, 3],\n",
       " [2, 239, 285, 19, 3],\n",
       " [2, 239, 285, 551, 3],\n",
       " [2, 239, 161, 5, 88, 3],\n",
       " [2, 239, 140, 105, 53, 203, 90, 21, 19, 3],\n",
       " [2, 239, 1214, 28, 6019, 2037, 3],\n",
       " [2, 239, 1214, 28, 16, 285, 88, 19, 3],\n",
       " [2, 239, 51, 57, 7, 153, 3],\n",
       " [2, 239, 51, 937, 110, 17, 19, 3],\n",
       " [2, 239, 966, 3],\n",
       " [2, 239, 795, 41, 3724, 3],\n",
       " [2, 239, 240, 1087, 88, 3],\n",
       " [2, 239, 6, 110, 9, 2684, 3],\n",
       " [2, 239, 6, 105, 4671, 62, 13, 19, 3],\n",
       " [2, 239, 6, 506, 67, 22, 37, 62, 23, 179, 19, 3],\n",
       " [2, 239, 6, 45, 223, 19, 3],\n",
       " [2, 2303, 2240, 386, 5, 88, 3],\n",
       " [2, 239, 22, 186, 15, 153, 3],\n",
       " [2, 239, 22, 186, 15, 13, 19, 3],\n",
       " [2, 239, 22, 833, 153, 3],\n",
       " [2, 239, 22, 1292, 15, 13, 3],\n",
       " [2, 55, 132, 3023, 98, 237, 3],\n",
       " [2, 55, 1589, 34, 36, 66, 7, 6, 14, 116, 153, 4, 3],\n",
       " [2, 55, 3427, 742, 26, 13, 3],\n",
       " [2, 55, 1212, 40, 3],\n",
       " [2, 55, 1704, 3],\n",
       " [2, 55, 1161, 9, 1306, 3],\n",
       " [2, 55, 1107, 2770, 34, 14, 33, 97, 3],\n",
       " [2, 55, 437, 3207, 17, 3],\n",
       " [2, 55, 437, 1562, 229, 14, 33, 199, 52, 5, 610, 3],\n",
       " [2, 55, 437, 1562, 69, 62, 13, 3],\n",
       " [2, 55, 4672, 7, 24, 685, 25, 3],\n",
       " [2, 55, 634, 3],\n",
       " [2, 55, 1385, 3],\n",
       " [2, 55, 57, 7, 6, 3428, 9, 1533, 13, 3],\n",
       " [2, 55, 3024, 354, 64, 478, 21, 3],\n",
       " [2, 55, 3024, 6020, 33, 21, 3],\n",
       " [2, 55, 3025, 22, 45, 10, 21, 3],\n",
       " [2, 55, 6021, 43, 3],\n",
       " [2, 55, 70, 40, 4, 1014, 13, 4, 3],\n",
       " [2, 55, 7, 43, 240, 3],\n",
       " [2, 1386, 285, 551, 19, 3],\n",
       " [2, 453, 76, 103, 62, 40, 4, 3],\n",
       " [2, 4673, 2304, 4674, 14, 33, 97, 3],\n",
       " [2, 6022, 1618, 60, 13, 3],\n",
       " [2, 6023, 122, 3],\n",
       " [2, 4675, 198, 82, 26, 20, 19, 3],\n",
       " [2, 941, 6, 24, 471, 10, 21, 3],\n",
       " [2, 356, 110, 225, 1338, 45, 9, 3],\n",
       " [2, 356, 57, 1620, 6, 28, 1339, 3],\n",
       " [2, 356, 57, 7, 11, 38, 13, 3],\n",
       " [2, 1338, 9, 11, 38, 13, 3],\n",
       " [2, 1338, 9, 41, 862, 122, 3],\n",
       " [2, 1338, 119, 14, 33, 113, 105, 1620, 20, 3],\n",
       " [2, 360, 115, 1988, 5, 45, 558, 3],\n",
       " [2, 941, 3429, 152, 129, 262, 400, 3],\n",
       " [2, 1906, 3273, 45, 673, 11, 728, 3],\n",
       " [2, 1906, 3430, 45, 168, 3],\n",
       " [2, 1802, 267, 590, 131, 206, 19, 3],\n",
       " [2, 4676, 4677, 19, 3],\n",
       " [2, 6782, 242, 64, 70, 40, 3],\n",
       " [2, 289, 11, 38, 40, 4, 3],\n",
       " [2, 289, 286, 4678, 3],\n",
       " [2, 289, 286, 9, 11, 38, 40, 111, 3],\n",
       " [2, 289, 286, 9, 64, 522, 10, 16, 532, 40, 3],\n",
       " [2, 289, 174, 284, 1017, 11, 38, 13, 3],\n",
       " [2, 289, 21, 203, 28, 5, 37, 13, 3],\n",
       " [2, 1134, 1856, 241, 549, 19, 3],\n",
       " [2, 1134, 1856, 90, 660, 701, 367, 3],\n",
       " [2, 1134, 2581, 9, 11, 38, 13, 3],\n",
       " [2, 1134, 2581, 9, 243, 17, 277, 19, 3],\n",
       " [2, 6024, 27, 46, 9, 37, 13, 3],\n",
       " [2, 3026, 7, 40, 2305, 13, 3],\n",
       " [2, 3026, 7, 286, 9, 866, 3],\n",
       " [2, 2674, 344, 6025, 7, 6, 28, 3],\n",
       " [2, 4679, 2603, 1936, 840, 3],\n",
       " [2, 4680, 3431, 4681, 3],\n",
       " [2, 285, 909, 87, 768, 310, 179, 179, 3],\n",
       " [2, 285, 23, 205, 14, 20, 19, 3],\n",
       " [2, 285, 138, 142, 131, 122, 3],\n",
       " [2, 2514, 89, 6, 36, 22, 478, 21, 3],\n",
       " [2, 3027, 41, 733, 223, 3],\n",
       " [2, 3027, 41, 223, 3],\n",
       " [2, 3028, 15, 25, 349, 5, 45, 169, 3],\n",
       " [2, 259, 140, 169, 3],\n",
       " [2, 259, 76, 167, 13, 3],\n",
       " [2, 259, 87, 45, 167, 80, 3],\n",
       " [2, 3616, 5, 2077, 3],\n",
       " [2, 529, 1799, 14, 33, 21, 3],\n",
       " [2, 2685, 2686, 549, 19, 3],\n",
       " [2, 2685, 2686, 7, 41, 6026, 19, 3],\n",
       " [2, 2685, 5, 1225, 3],\n",
       " [2, 259, 5, 211, 3725, 17, 3],\n",
       " [2, 259, 5, 55, 3432, 13, 3],\n",
       " [2, 259, 5, 610, 3],\n",
       " [2, 259, 5, 4682, 17, 3],\n",
       " [2, 4683, 571, 37, 23, 19, 3],\n",
       " [2, 6027, 2765, 13, 3],\n",
       " [2, 386, 5, 167, 13, 3],\n",
       " [2, 1168, 4684, 7, 24, 134, 64, 212, 43, 3],\n",
       " [2, 679, 5, 37, 6, 14, 33, 21, 3],\n",
       " [2, 370, 24, 210, 204, 103, 23, 18, 40, 3],\n",
       " [2, 370, 13, 20, 11, 15, 25, 1217, 5, 47, 45, 23, 3],\n",
       " [2, 3726, 144, 4685, 1392, 9, 15, 6, 448, 3],\n",
       " [2, 40, 423, 230, 11, 38, 40, 3],\n",
       " [2, 40, 23, 10, 21, 60, 80, 10, 26, 13, 3],\n",
       " [2, 40, 2926, 3650, 11, 38, 13, 3],\n",
       " [2, 40, 90, 11, 15, 93, 19, 3],\n",
       " [2, 40, 57, 17, 3],\n",
       " [2, 40, 124, 37, 13, 3],\n",
       " [2, 247, 6028, 3],\n",
       " [2, 132, 14, 194, 17, 277, 3],\n",
       " [2, 132, 83, 6783, 683, 1327, 750, 17, 3],\n",
       " [2, 132, 28, 69, 94, 30, 8, 2353, 11, 38, 13, 3],\n",
       " [2, 132, 28, 69, 16, 166, 122, 3],\n",
       " [2, 132, 28, 44, 1607, 5, 55, 248, 1621, 3],\n",
       " [2, 132, 6029, 840, 3],\n",
       " [2, 1443, 4686, 41, 45, 54, 25, 485, 45, 4687, 3],\n",
       " [2, 1443, 2048, 6, 14, 51, 69, 304, 3],\n",
       " [2, 1443, 1341, 41, 45, 10, 21, 19, 3],\n",
       " [2, 1443, 9, 6030, 901, 3],\n",
       " [2, 1443, 9, 1572, 16, 14, 33, 21, 3],\n",
       " [2, 125, 89, 726, 83, 627, 18, 11, 38, 13, 3],\n",
       " [2, 125, 1803, 22, 6031, 1917, 6032, 19, 3],\n",
       " [2, 3433, 5, 87, 1058, 13, 277, 3],\n",
       " [2, 3433, 5, 2346, 18, 499, 3],\n",
       " [2, 436, 27, 140, 89, 223, 3],\n",
       " [2, 1178, 562, 7, 11, 38, 113, 3],\n",
       " [2, 4688, 3717, 4689, 4, 3],\n",
       " [2, 6033, 2306, 134, 11, 38, 13, 3],\n",
       " [2, 824, 442, 5, 6, 409, 3],\n",
       " [2, 4690, 3029, 19, 3],\n",
       " [2, 2370, 3],\n",
       " [2, 2370, 967, 3],\n",
       " [2, 3434, 594, 76, 1048, 3],\n",
       " [2, 3434, 594, 1402, 5, 45, 168, 3],\n",
       " [2, 1354, 34, 6034, 103, 11, 38, 40, 3],\n",
       " [2, 1354, 34, 106, 7, 11, 38, 13, 3],\n",
       " [2, 2355, 5, 140, 2011, 43, 3],\n",
       " [2, 1266, 34, 4691, 103, 11, 38, 13, 3],\n",
       " [2, 953, 55, 363, 2687, 3],\n",
       " [2, 953, 2525, 819, 3],\n",
       " [2, 953, 675, 2485, 25, 953, 971, 11, 38, 40, 3],\n",
       " [2, 4692, 105, 7, 20, 19, 3],\n",
       " [2, 476, 5, 45, 169, 3],\n",
       " [2, 476, 5, 287, 15, 93, 19, 3],\n",
       " [2, 476, 4693, 45, 89, 6, 146, 15, 23, 4, 3],\n",
       " [2, 6035, 3435, 13, 3],\n",
       " [2, 1003, 7, 64, 9, 634, 3],\n",
       " [2, 367, 3030, 116, 251, 216, 19, 3],\n",
       " [2, 367, 3030, 1188, 1678, 499, 3],\n",
       " [2, 4694, 51, 60, 80, 10, 26, 13, 3],\n",
       " [2, 1613, 4695, 25, 45, 163, 43, 3],\n",
       " [2, 1613, 326, 16, 4696, 6, 24, 731, 98, 237, 3],\n",
       " [2, 1613, 1822, 3727, 4697, 3],\n",
       " [2, 4698, 289, 286, 9, 11, 38, 13, 3],\n",
       " [2, 6036, 426, 2307, 6037, 3],\n",
       " [2, 4699, 1936, 7, 40, 388, 13, 3],\n",
       " [2, 2366, 9, 464, 88, 3],\n",
       " [2, 3264, 728, 3],\n",
       " [2, 1693, 3031, 34, 28, 5, 54, 11, 38, 13, 3],\n",
       " [2, 2012, 1135, 40, 355, 26, 13, 3],\n",
       " [2, 2012, 276, 40, 1135, 551, 3],\n",
       " [2, 2012, 6038, 137, 320, 76, 593, 20, 19, 3],\n",
       " [2, 2012, 99, 444, 82, 93, 3],\n",
       " [2, 3415, 60, 193, 6039, 13, 3],\n",
       " [2, 1275, 9, 11, 38, 40, 4, 3],\n",
       " [2, 1275, 9, 6, 24, 581, 20, 19, 3],\n",
       " [2, 1275, 9, 41, 10, 26, 20, 3],\n",
       " [2, 1275, 1777, 14, 1339, 3],\n",
       " [2, 1275, 40, 1204, 13, 3],\n",
       " [2, 1275, 85, 4700, 115, 216, 19, 3],\n",
       " [2, 3032, 9, 11, 38, 13, 3],\n",
       " [2, 63, 3728, 7, 24, 506, 36, 22, 45, 7, 11, 38, 40, 4, 3],\n",
       " [2, 63, 23, 16, 647, 594, 7, 11, 38, 13, 3],\n",
       " [2, 63, 268, 23, 78, 45, 167, 80, 10, 26, 13, 3],\n",
       " [2, 63, 268, 110, 53, 14, 33, 21, 3],\n",
       " [2, 4701, 311, 1186, 228, 449, 11, 38, 20, 3],\n",
       " [2, 1705, 3],\n",
       " [2, 1705, 742, 26, 40, 3],\n",
       " [2, 1705, 355, 26, 40, 3],\n",
       " [2, 1705, 1705, 3],\n",
       " [2, 3427, 349, 8, 6040, 3],\n",
       " [2, 4702, 4703, 119, 62, 13, 3],\n",
       " [2, 2526, 382, 155, 3],\n",
       " [2, 368, 910, 6, 552, 3],\n",
       " [2, 1218, 56, 1191, 2929, 422, 4704, 3],\n",
       " [2, 1218, 56, 20, 22, 49, 11, 3],\n",
       " [2, 1329, 1074, 5, 37, 13, 3],\n",
       " [2, 984, 9, 149, 337, 728, 3],\n",
       " [2, 984, 4705, 28, 82, 40, 4, 3],\n",
       " [2, 984, 844, 4706, 45, 3033, 40, 3],\n",
       " [2, 984, 27, 844, 3618, 286, 9, 329, 3],\n",
       " [2, 2527, 1284, 441, 11, 4707, 3],\n",
       " [2, 2527, 1284, 6, 14, 478, 16, 139, 191, 103, 93, 3],\n",
       " [2, 2527, 1284, 149, 289, 286, 9, 11, 38, 13, 3],\n",
       " [2, 534, 564, 28, 69, 55, 1446, 40, 4, 3],\n",
       " [2, 4708, 33, 21, 3],\n",
       " [2, 4533, 7, 6041, 70, 13, 3],\n",
       " [2, 284, 441, 11, 6784, 2455, 3],\n",
       " [2, 284, 4709, 64, 3],\n",
       " [2, 284, 40, 287, 521, 23, 3],\n",
       " [2, 284, 40, 37, 13, 509, 13, 3],\n",
       " [2, 284, 95, 80, 10, 26, 20, 19, 3],\n",
       " [2, 284, 95, 16, 3015, 7, 11, 38, 13, 3],\n",
       " [2, 284, 76, 1017, 11, 38, 13, 3],\n",
       " [2, 284, 2052, 11, 38, 13, 3],\n",
       " [2, 284, 2052, 6, 707, 3],\n",
       " [2, 284, 1644, 6, 24, 233, 20, 49, 304, 3],\n",
       " [2, 284, 1017, 41, 105, 206, 3],\n",
       " [2, 284, 1300, 479, 25, 65, 4710, 3],\n",
       " [2, 284, 45, 397, 6, 241, 1686, 3],\n",
       " [2, 284, 287, 521, 20, 3],\n",
       " [2, 284, 37, 25, 4711, 4712, 254, 131, 122, 3],\n",
       " [2, 284, 37, 13, 3],\n",
       " [2, 284, 3034, 13, 3],\n",
       " [2, 284, 87, 4713, 3],\n",
       " [2, 284, 460, 142, 7, 64, 207, 19, 3],\n",
       " [2, 284, 16, 133, 2052, 6, 14, 629, 3],\n",
       " [2, 284, 5, 3035, 612, 1102, 37, 13, 3],\n",
       " [2, 284, 5, 206, 22, 37, 13, 3],\n",
       " [2, 1660, 26, 40, 3],\n",
       " [2, 6042, 1622, 3],\n",
       " [2, 3436, 9, 23, 181, 190, 1567, 3],\n",
       " [2, 4714, 8, 17, 4, 3],\n",
       " [2, 6043, 1423, 1618, 60, 13, 3],\n",
       " [2, 1199, 5, 23, 4715, 11, 142, 7, 26, 367, 3],\n",
       " [2, 1199, 158, 4716, 1156, 3],\n",
       " [2, 1199, 78, 3437, 4717, 13, 3],\n",
       " [2, 2013, 46, 69, 5, 3025, 3],\n",
       " [2, 1982, 1622, 3],\n",
       " [2, 1982, 354, 64, 478, 21, 3],\n",
       " [2, 1982, 354, 20, 51, 499, 3],\n",
       " [2, 54, 6, 67, 5, 206, 22, 37, 43, 3],\n",
       " [2, 4718, 21, 18, 20, 51, 11, 354, 88, 7, 25, 3],\n",
       " [2, 758, 27, 999, 330, 163, 6, 36, 33, 21, 966, 3],\n",
       " [2, 6785, 40, 3726, 223, 24, 54, 43, 3],\n",
       " [2, 3328, 3329, 197, 81, 13, 3],\n",
       " [2, 877, 241, 152, 105, 7, 20, 3],\n",
       " [2, 877, 772, 4719, 3],\n",
       " [2, 877, 4720, 3],\n",
       " [2, 877, 889, 1847, 99, 81, 13, 3],\n",
       " [2, 877, 2528, 296, 840, 3],\n",
       " [2, 1143, 408, 24, 37, 13, 3],\n",
       " [2, 1143, 23, 18, 174, 289, 11, 38, 40, 4, 3],\n",
       " [2, 6044, 57, 45, 54, 43, 3],\n",
       " [2, 1928, 241, 131, 26, 40, 3],\n",
       " [2, 1882, 4721, 273, 11, 38, 40, 3],\n",
       " [2, 6786, 2159, 6787, 19, 3],\n",
       " [2, 4722, 55, 819, 40, 4, 1883, 4723, 3],\n",
       " [2, 2372, 122, 3],\n",
       " [2, 2161, 225, 10, 40, 4, 3],\n",
       " [2, 578, 34, 196, 5, 45, 558, 3],\n",
       " [2, 522, 10, 21, 3],\n",
       " [2, 1806, 103, 11, 38, 40, 4, 3],\n",
       " [2, 1806, 4724, 9, 11, 38, 13, 3],\n",
       " [2, 2034, 2181, 10, 26, 40, 4, 3],\n",
       " [2, 682, 91, 6045, 3],\n",
       " [2, 3438, 103, 11, 38, 40, 3],\n",
       " [2, 1091, 23, 179, 3],\n",
       " [2, 4725, 695, 3036, 1878, 13, 3],\n",
       " [2, 140, 1701, 5, 1157, 3],\n",
       " [2, 140, 337, 131, 168, 3],\n",
       " [2, 140, 110, 388, 13, 3],\n",
       " [2, 140, 575, 103, 23, 3],\n",
       " [2, 140, 2999, 14, 33, 21, 3],\n",
       " [2, 140, 3439, 374, 3],\n",
       " [2, 140, 1307, 3],\n",
       " [2, 140, 133, 1598, 20, 19, 3],\n",
       " [2, 140, 374, 45, 99, 21, 3],\n",
       " [2, 140, 3729, 7, 43, 3],\n",
       " [2, 140, 4726, 9, 55, 95, 21, 3],\n",
       " [2, 3037, 4516, 24, 51, 728, 3],\n",
       " [2, 323, 16, 1836, 69, 3],\n",
       " [2, 1643, 7, 41, 10, 26, 40, 3],\n",
       " [2, 6046, 4727, 3],\n",
       " [2, 3440, 44, 186, 8, 7, 11, 15, 13, 3],\n",
       " [2, 3730, 3730, 72, 289, 21, 56, 12, 3],\n",
       " [2, 357, 489, 24, 4728, 6, 552, 1160, 3],\n",
       " [2, 357, 103, 11, 3038, 3],\n",
       " [2, 357, 103, 11, 38, 13, 3],\n",
       " [2, 4729, 191, 11, 38, 113, 819, 3],\n",
       " [2, 3212, 394, 34, 14, 33, 113, 137, 1295, 5, 37, 20, 19, 3],\n",
       " [2, 6047, 3441, 179, 3],\n",
       " [2, 4730, 4731, 220, 45, 23, 3],\n",
       " [2, 1921, 2529, 759, 6048, 3],\n",
       " [2, 1921, 2529, 759, 31, 1306, 56, 12, 3],\n",
       " [2, 1921, 2529, 87, 252, 80, 3],\n",
       " [2, 1858, 6049, 1480, 3],\n",
       " [2, 4732, 4733, 3442, 122, 3],\n",
       " [2, 4734, 4735, 2688, 103, 11, 38, 40, 4, 3],\n",
       " [2, 1124, 2530, 17, 277, 3],\n",
       " [2, 6050, 354, 277, 3],\n",
       " [2, 3013, 3412, 558, 3],\n",
       " [2, 1296, 2531, 4736, 2783, 60, 13, 3],\n",
       " [2, 52, 40, 321, 6, 155, 3],\n",
       " [2, 52, 22, 1742, 11, 532, 22, 1742, 43, 3],\n",
       " [2, 52, 66, 16, 3214, 5, 88, 3],\n",
       " [2, 52, 27, 397, 6, 14, 33, 113, 160, 8, 137, 45, 7, 20, 3],\n",
       " [2, 52, 27, 45, 397, 6, 831, 66, 17, 3],\n",
       " [2, 52, 8, 576, 56, 11, 38, 13, 4, 3],\n",
       " [2, 52, 8, 141, 7, 11, 15, 25, 3440, 44, 59, 9, 250, 8, 60, 43, 3],\n",
       " [2, 52, 44, 700, 9, 37, 13, 3],\n",
       " [2, 52, 44, 3701, 9, 124, 17, 3],\n",
       " [2, 52, 5, 4737, 3],\n",
       " [2, 52, 5, 520, 17, 3],\n",
       " [2, 52, 5, 1518, 225, 222, 450, 8, 242, 24, 54, 43, 3],\n",
       " [2, 52, 5, 1309, 17, 3],\n",
       " [2, 52, 5, 222, 4738, 3],\n",
       " [2, 52, 5, 4739, 3],\n",
       " [2, 1105, 105, 191, 11, 38, 13, 4, 3],\n",
       " [2, 1377, 87, 45, 60, 80, 10, 26, 40, 3],\n",
       " [2, 1377, 1387, 5, 20, 3],\n",
       " [2, 1377, 53, 14, 33, 113, 3],\n",
       " [2, 2929, 3413, 40, 3],\n",
       " [2, 89, 64, 66, 7, 41, 1551, 3],\n",
       " [2, 89, 6, 24, 171, 20, 6, 49, 21, 3],\n",
       " [2, 89, 41, 1551, 3],\n",
       " [2, 89, 41, 10, 271, 683, 478, 21, 3],\n",
       " [2, 89, 41, 10, 16, 28, 5, 15, 13, 4, 4740, 3],\n",
       " [2, 6051, 1866, 5, 43, 111, 3],\n",
       " [2, 4741, 103, 11, 38, 13, 3],\n",
       " [2, 1573, 8, 114, 23, 179, 3],\n",
       " [2, 1573, 131, 820, 19, 3],\n",
       " [2, 76, 103, 62, 25, 2308, 3037, 33, 21, 3],\n",
       " [2, 76, 103, 62, 678, 1428, 3],\n",
       " [2, 76, 3039, 670, 17, 3],\n",
       " [2, 51, 14, 6, 24, 634, 3],\n",
       " [2, 51, 57, 7, 11, 38, 13, 3],\n",
       " [2, 4742, 5, 3040, 3],\n",
       " [2, 51, 519, 60, 40, 3],\n",
       " [2, 51, 8, 110, 7, 26, 13, 3],\n",
       " [2, 51, 8, 6052, 1304, 6, 28, 133, 17, 19, 3],\n",
       " [2, 51, 8, 597, 24, 7, 41, 10, 2089, 3],\n",
       " [2, 51, 8, 137, 320, 110, 90, 21, 449, 153, 3],\n",
       " [2, 51, 8, 17, 422, 11, 175, 168, 355, 26, 13, 3],\n",
       " [2, 51, 5, 55, 924, 3],\n",
       " [2, 51, 5, 45, 1462, 3],\n",
       " [2, 51, 5, 45, 168, 3],\n",
       " [2, 51, 5, 471, 1186, 237, 3],\n",
       " [2, 3217, 728, 3],\n",
       " [2, 956, 9, 255, 27, 45, 69, 13, 3],\n",
       " [2, 51, 7, 11, 23, 153, 674, 7, 1440, 3],\n",
       " [2, 51, 7, 11, 175, 7, 41, 312, 17, 3],\n",
       " [2, 51, 7, 6, 394, 40, 54, 41, 135, 10, 26, 40, 3],\n",
       " [2, 51, 7, 6, 394, 899, 13, 1599, 4, 3],\n",
       " [2, 51, 7, 20, 51, 119, 3],\n",
       " [2, 51, 17, 179, 3],\n",
       " [2, 489, 6, 14, 103, 11, 38, 113, 134, 4743, 267, 168, 3],\n",
       " [2, 1857, 9, 11, 38, 13, 3],\n",
       " [2, 1857, 2532, 13, 3],\n",
       " [2, 6053, 6054, 489, 40, 3],\n",
       " [2, 197, 6, 331, 391, 3],\n",
       " [2, 2298, 76, 3041, 6, 28, 3],\n",
       " [2, 3443, 3444, 103, 11, 38, 40, 4, 3],\n",
       " [2, 3443, 6055, 6056, 19, 3],\n",
       " [2, 411, 366, 670, 17, 3],\n",
       " [2, 411, 120, 6, 1299, 85, 18, 6, 92, 28, 3],\n",
       " [2, 6057, 34, 3444, 342, 3],\n",
       " [2, 1778, 1623, 20, 3],\n",
       " [2, 1778, 2533, 2865, 2533, 3],\n",
       " [2, 1778, 34, 671, 549, 19, 3],\n",
       " [2, 626, 323, 21, 3],\n",
       " [2, 626, 323, 97, 3],\n",
       " [2, 626, 626, 323, 21, 3],\n",
       " [2, 336, 3010, 4744, 14, 3445, 23, 19, 3],\n",
       " [2, 336, 1807, 11, 4745, 3],\n",
       " [2, 336, 1807, 607, 1616, 4, 3],\n",
       " [2, 336, 1380, 150, 345, 3],\n",
       " [2, 336, 6058, 13, 145, 127, 1706, 19, 3],\n",
       " [2, 336, 140, 6059, 168, 3],\n",
       " [2, 336, 2309, 3],\n",
       " [2, 336, 610, 3],\n",
       " [2, 336, 524, 14, 33, 97, 3],\n",
       " [2, 336, 45, 418, 81, 678, 2519, 509, 13, 3],\n",
       " [2, 336, 133, 1807, 93, 19, 3],\n",
       " [2, 336, 133, 122, 19, 3],\n",
       " [2, 336, 1648, 728, 3],\n",
       " [2, 336, 456, 24, 4746, 3],\n",
       " [2, 336, 206, 22, 45, 71, 8, 36, 33, 21, 3],\n",
       " [2, 336, 34, 367, 197, 16, 14, 33, 21, 3],\n",
       " [2, 336, 9, 4747, 24, 34, 367, 197, 16, 14, 33, 21, 3],\n",
       " [2, 336, 9, 3446, 3],\n",
       " [2, 336, 9, 4748, 3042, 3],\n",
       " [2, 336, 66, 1704, 3],\n",
       " [2, 336, 849, 1451, 277, 19, 3],\n",
       " [2, 2218, 76, 2097, 3],\n",
       " [2, 2218, 5, 560, 2097, 3],\n",
       " [2, 4749, 87, 4750, 80, 10, 26, 13, 4, 3],\n",
       " [2, 1367, 5, 520, 17, 3],\n",
       " [2, 1367, 5, 6060, 13, 3],\n",
       " [2, 103, 11, 223, 11, 289, 11, 66, 38, 13, 3],\n",
       " [2, 103, 11, 3039, 289, 64, 66, 115, 216, 3],\n",
       " [2, 103, 64, 22, 478, 21, 3],\n",
       " [2, 103, 8, 14, 105, 37, 23, 3],\n",
       " [2, 103, 8, 14, 206, 22, 37, 13, 3],\n",
       " ...]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train = tf.keras.preprocessing.sequence.pad_sequences(sou, maxlen = MAX_LENGTH, padding='post')\n",
    "dec_train = tf.keras.preprocessing.sequence.pad_sequences(tgt, maxlen = MAX_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2, 2512,  214, 3717,  111,    3,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2, 302,   9, 140,   9,  42,   4,   3,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델링\n",
    "\n",
    "\n",
    "``` code\n",
    "# 예문\n",
    "1. 지루하다, 놀러가고 싶어.\n",
    "2. 오늘 일찍 일어났더니 피곤하다.\n",
    "3. 간만에 여자친구랑 데이트 하기로 했어.\n",
    "4. 집에 있는다는 소리야.\n",
    "\n",
    "---\n",
    "\n",
    "# 제출\n",
    "\n",
    "Translations\n",
    "> 1. 잠깐 쉬 어도 돼요 . <end>\n",
    "> 2. 맛난 거 드세요 . <end>\n",
    "> 3. 떨리 겠 죠 . <end>\n",
    "> 4. 좋 아 하 면 그럴 수 있 어요 . <end>\n",
    "\n",
    "Hyperparameters\n",
    "> n_layers: 1\n",
    "> d_model: 368\n",
    "> n_heads: 8\n",
    "> d_ff: 1024\n",
    "> dropout: 0.2\n",
    "\n",
    "Training Parameters\n",
    "> Warmup Steps: 1000\n",
    "> Batch Size: 64\n",
    "> Epoch At: 10\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "\n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "\n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "\n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "\n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.dec_self_attn(out, enc_out, enc_out, causality_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "\n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "\n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "\n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "\n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "\n",
    "        logits = self.fc(dec_out)\n",
    "\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\n",
    "d_model = 512\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]  # Decoder의 input\n",
    "    gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssac4/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e69bfb5a45734a1facd3ab77faa51f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/328 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c00e862f68db4b30a6876789e3c6d5d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/328 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01faba99f2ff4b08af55dd67a76c689e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/328 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c4e53a5b9054ff0a3018bef8e22b641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/328 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57aed9b66c6644df82803de1aa1e2a9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/328 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f5d86214ef457887d82e9d9cb99e32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/328 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833b4b81ed344567907ead5e35d067ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/328 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e66a93b58fd44d0b147d51b4389b1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/328 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc371be58cc14fde897a2452c17c6e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/328 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7330dd0e3c204d7b813a5c01d2982f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/328 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook \n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm_notebook(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                    dec_train[idx:idx+BATCH_SIZE],\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 대화 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = ['지루하다, 놀러가고 싶어.','오늘 일찍 일어났더니 피곤하다.',\n",
    "            '간만에 여자친구랑 데이트 하기로 했어.','집에 있는다는 소리야.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "transformer() got an unexpected keyword argument 'inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-251-107b110ad4fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msentence_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-214-50a1fc8919b1>\u001b[0m in \u001b[0;36msentence_generation\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msentence_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# 입력 문장에 대해서 디코더를 동작시켜 예측된 정수 시퀀스를 리턴받습니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprediction_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-213-985172f0e59e>\u001b[0m in \u001b[0;36mdecoder_inference\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mincoded_sen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_sequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: transformer() got an unexpected keyword argument 'inputs'"
     ]
    }
   ],
   "source": [
    "for example in examples:\n",
    "    sentence_generation(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLUE Score \n",
    "\n",
    "Bilingual Evaluation Understudy score \n",
    "\n",
    "[참고 자료](https://donghwa-kim.github.io/BLEU.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문: ['많', '은', '자연어', '처리', '연구자', '들', '이', '트랜스포머', '를', '선호', '한다']\n",
      "번역문: ['적', '은', '자연어', '학', '개발자', '들', '가', '트랜스포머', '을', '선호', '한다', '요']\n",
      "BLEU Score: 8.190757052088229e-155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssac4/anaconda3/envs/aiffel/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/ssac4/anaconda3/envs/aiffel/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# !pip install nltk # nltk가 설치되어 있지 않은 경우 주석 해제\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "reference = \"많 은 자연어 처리 연구자 들 이 트랜스포머 를 선호 한다\".split()\n",
    "candidate = \"적 은 자연어 학 개발자 들 가 트랜스포머 을 선호 한다 요\".split()\n",
    "\n",
    "print(\"원문:\", reference)\n",
    "print(\"번역문:\", candidate)\n",
    "print(\"BLEU Score:\", sentence_bleu([reference], candidate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram: 0.5\n",
      "2-gram: 0.18181818181818182\n",
      "3-gram: 2.2250738585072626e-308\n",
      "4-gram: 2.2250738585072626e-308\n"
     ]
    }
   ],
   "source": [
    "print(\"1-gram:\", sentence_bleu([reference], candidate, weights=[1, 0, 0, 0]))\n",
    "print(\"2-gram:\", sentence_bleu([reference], candidate, weights=[0, 1, 0, 0]))\n",
    "print(\"3-gram:\", sentence_bleu([reference], candidate, weights=[0, 0, 1, 0]))\n",
    "print(\"4-gram:\", sentence_bleu([reference], candidate, weights=[0, 0, 0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "(\\prod_{i=1}^4 precision_i)^{\\frac{1}{4}} = (\\text{1-gram} \\times\\text{2-gram} \\times\\text{3-gram} \\times\\text{4-gram})^{\\frac{1}{4}}\n",
    "$$\n",
    "\n",
    "3gram, 4gram에서 0에 가까운 점수를 받아서 BLUE SCORE가 지나치게 낮음.\n",
    "\n",
    "곱연산후 루트를 씌우기때문에 하나의 값이 0이면 지나치게 총 BLUE가 작아지기 때문\n",
    "\n",
    "가끔 0으로 계산되는 값을 1.0으로 보정해서 게산하기도 했는데 1.0은 완벽한 번역을의미하기때문에 총점이 이상하게 높아질수가 있음.\n",
    "\n",
    "즉 0,1 의 값들 때문에 값이 왜곡되는 경우가 많았음. 이를 방지하기위해\n",
    "\n",
    "이런 걸 방지하기 위해 SmoothingFunction으로 epsilon을 더해줘서 0에 가까운 값도 보정해줘서 해결함.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.5\n",
      "BLEU-2: 0.18181818181818182\n",
      "BLEU-3: 0.010000000000000004\n",
      "BLEU-4: 0.011111111111111112\n",
      "\n",
      "BLEU-Total: 0.05637560315259291\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    return sentence_bleu([reference],\n",
    "                         candidate,\n",
    "                         weights=weights,\n",
    "                         smoothing_function=SmoothingFunction().method1)  # smoothing_function 적용\n",
    "\n",
    "print(\"BLEU-1:\", calculate_bleu(reference, candidate, weights=[1, 0, 0, 0]))\n",
    "print(\"BLEU-2:\", calculate_bleu(reference, candidate, weights=[0, 1, 0, 0]))\n",
    "print(\"BLEU-3:\", calculate_bleu(reference, candidate, weights=[0, 0, 1, 0]))\n",
    "print(\"BLEU-4:\", calculate_bleu(reference, candidate, weights=[0, 0, 0, 1]))\n",
    "\n",
    "print(\"\\nBLEU-Total:\", calculate_bleu(reference, candidate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 조금 더 해석하기 편리하게 BLUE 계산되었음 이때 사용하는 SmothingFunction()은 디폴트가 mothod0이며 7까지 존재함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Search Decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def beam_search_decoder(prob, beam_size):\n",
    "    sequences = [[[], 1.0]]  # 생성된 문장과 점수를 저장\n",
    "\n",
    "    for tok in prob:\n",
    "        all_candidates = []\n",
    "\n",
    "        for seq, score in sequences:\n",
    "            for idx, p in enumerate(tok): # 각 단어의 확률을 총점에 누적 곱\n",
    "                candidate = [seq + [idx], score * -math.log(-(p-1))]\n",
    "                all_candidates.append(candidate)\n",
    "\n",
    "        ordered = sorted(all_candidates,\n",
    "                         key=lambda tup:tup[1],\n",
    "                         reverse=True) # 총점 순 정렬\n",
    "        sequences = ordered[:beam_size] # Beam Size에 해당하는 문장만 저장 \n",
    "\n",
    "    return sequences\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "커피 를 가져 도 될 까요? <pad> <pad> <pad> <pad>  // Score: 42.5243\n",
      "커피 를 마셔 도 될 까요? <pad> <pad> <pad> <pad>  // Score: 28.0135\n",
      "마셔 를 가져 도 될 까요? <pad> <pad> <pad> <pad>  // Score: 17.8983\n"
     ]
    }
   ],
   "source": [
    "vocab = {\n",
    "    0: \"<pad>\",\n",
    "    1: \"까요?\",\n",
    "    2: \"커피\",\n",
    "    3: \"마셔\",\n",
    "    4: \"가져\",\n",
    "    5: \"될\",\n",
    "    6: \"를\",\n",
    "    7: \"한\",\n",
    "    8: \"잔\",\n",
    "    9: \"도\",\n",
    "}\n",
    "\n",
    "prob_seq = [[0.01, 0.01, 0.60, 0.32, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.75, 0.01, 0.01, 0.17],\n",
    "            [0.01, 0.01, 0.01, 0.35, 0.48, 0.10, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.24, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.68],\n",
    "            [0.01, 0.01, 0.12, 0.01, 0.01, 0.80, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.01, 0.81, 0.01, 0.01, 0.01, 0.01, 0.11, 0.01, 0.01, 0.01],\n",
    "            [0.70, 0.22, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]]\n",
    "\n",
    "prob_seq = np.array(prob_seq)\n",
    "beam_size = 3\n",
    "\n",
    "result = beam_search_decoder(prob_seq, beam_size)\n",
    "\n",
    "for seq, score in result:\n",
    "    sentence = \"\"\n",
    "\n",
    "    for word in seq:\n",
    "        sentence += vocab[word] + \" \"\n",
    "\n",
    "    print(sentence, \"// Score: %.4f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사실 위 예시는 Beam Search 설명에는 적당한데 실제 모델이 문장 생성하는 과정과는 거리가 멈.     \n",
    "prob_seq처럼 확률을 정의하기가 힘듬. 한번에 정의되지않고 이전 스텝의 단어에 따라서 결정되기 때문임.     \n",
    "지금은 그런걸 고려하지않고 그냥 3번째 단어는 마신다 0.3 먹는다 0.5,... 의 확률을 가진다고 적어놨음.    \n",
    "  \n",
    "그래서 실제로 Beam Search를 생성긱법으로 구현할때는 분기를 잘 나눠줘어야함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_prob(src_ids, tgt_ids, model):\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "    generate_masks(src_ids, tgt_ids)\n",
    "\n",
    "    predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "    model(src_ids, \n",
    "            tgt_ids,\n",
    "            enc_padding_mask,\n",
    "            combined_mask,\n",
    "            dec_padding_mask)\n",
    "\n",
    "    return tf.math.softmax(predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decoder(sentence, \n",
    "                        src_len,\n",
    "                        tgt_len,\n",
    "                        model,\n",
    "                        src_tokenizer,\n",
    "                        tgt_tokenizer,\n",
    "                        beam_size):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    src_in = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                            maxlen=src_len,\n",
    "                                                            padding='post')\n",
    "\n",
    "    pred_cache = np.zeros((beam_size * beam_size, tgt_len), dtype=np.long)\n",
    "    pred = np.zeros((beam_size, tgt_len), dtype=np.long)\n",
    "\n",
    "    eos_flag = np.zeros((beam_size, ), dtype=np.long)\n",
    "    scores = np.ones((beam_size, ))\n",
    "\n",
    "    pred[:, 0] = tgt_tokenizer.bos_id()\n",
    "\n",
    "    dec_in = tf.expand_dims(pred[0, :1], 0)\n",
    "    prob = calc_prob(src_in, dec_in, model)[0, -1].numpy()\n",
    "\n",
    "    for seq_pos in range(1, tgt_len):\n",
    "        score_cache = np.ones((beam_size * beam_size, ))\n",
    "\n",
    "        # init\n",
    "        for branch_idx in range(beam_size):\n",
    "            cache_pos = branch_idx*beam_size\n",
    "\n",
    "            score_cache[cache_pos:cache_pos+beam_size] = scores[branch_idx]\n",
    "            pred_cache[cache_pos:cache_pos+beam_size, :seq_pos] = \\\n",
    "            pred[branch_idx, :seq_pos]\n",
    "\n",
    "        for branch_idx in range(beam_size):\n",
    "            cache_pos = branch_idx*beam_size\n",
    "\n",
    "            if seq_pos != 1:   # 모든 Branch를 <BOS>로 시작하는 경우를 방지\n",
    "                dec_in = pred_cache[branch_idx, :seq_pos]\n",
    "                dec_in = tf.expand_dims(dec_in, 0)\n",
    "\n",
    "                prob = calc_prob(src_in, dec_in, model)[0, -1].numpy()\n",
    "\n",
    "            for beam_idx in range(beam_size):\n",
    "                max_idx = np.argmax(prob)\n",
    "\n",
    "                score_cache[cache_pos+beam_idx] *= prob[max_idx]\n",
    "                pred_cache[cache_pos+beam_idx, seq_pos] = max_idx\n",
    "\n",
    "                prob[max_idx] = -1\n",
    "\n",
    "        for beam_idx in range(beam_size):\n",
    "            if eos_flag[beam_idx] == -1: continue\n",
    "\n",
    "            max_idx = np.argmax(score_cache)\n",
    "            prediction = pred_cache[max_idx, :seq_pos+1]\n",
    "\n",
    "            pred[beam_idx, :seq_pos+1] = prediction\n",
    "            scores[beam_idx] = score_cache[max_idx]\n",
    "            score_cache[max_idx] = -1\n",
    "\n",
    "            if prediction[-1] == tgt_tokenizer.eos_id():\n",
    "                eos_flag[beam_idx] = -1\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    return sentence_bleu([reference],\n",
    "                            candidate,\n",
    "                            weights=weights,\n",
    "                            smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "def beam_bleu(reference, ids, tokenizer):\n",
    "    reference = reference.split()\n",
    "\n",
    "    total_score = 0.0\n",
    "    for _id in ids:\n",
    "        candidate = tokenizer.decode_ids(_id.tolist()).split()\n",
    "        score = calculate_bleu(reference, candidate)\n",
    "\n",
    "        print(\"Reference:\", reference)\n",
    "        print(\"Candidate:\", candidate)\n",
    "        print(\"BLEU:\", calculate_bleu(reference, candidate))\n",
    "\n",
    "        total_score += score\n",
    "\n",
    "    return total_score / len(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 소고\n",
    "\n",
    "\n",
    "너무 어렵다.. 일단 학습은 시킨거같은데 이걸로 어떻게 만드는지 잘모르겠다.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
